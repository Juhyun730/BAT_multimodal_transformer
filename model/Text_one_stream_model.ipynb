{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"126yRrxBFXVe1xYViHWtT4ojgIdqvDCje","authorship_tag":"ABX9TyNvCANQk9k6p8lhZEwX0hDw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow import nn\n","import pandas as pd\n","import numpy as np\n","import pickle"],"metadata":{"id":"yO5AMxfQfhnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGb_KJJDbwDy"},"outputs":[],"source":["with open('/content/drive/MyDrive/kcc_휴먼이해논문공모전/Text_x_train.pkl', \"rb\") as f:\n","    x_train = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/kcc_휴먼이해논문공모전/Text_y_train.pkl', \"rb\") as f:\n","    y_train = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/kcc_휴먼이해논문공모전/Text_x_valid.pkl', \"rb\") as f:\n","    x_valid = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/kcc_휴먼이해논문공모전/Text_y_valid.pkl', \"rb\") as f:\n","    y_valid = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/kcc_휴먼이해논문공모전/Text_x_test.pkl', \"rb\") as f:\n","    x_test = pickle.load(f)\n","\n","with open('/content/drive/MyDrive/kcc_휴먼이해논문공모전/Text_y_test.pkl', \"rb\") as f:\n","    y_test = pickle.load(f)"]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","y_valid = to_categorical(y_valid)\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"metadata":{"id":"_yFt-tiFdC_6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","x_train = pad_sequences(x_train, maxlen=50)\n","x_test = pad_sequences(x_test, maxlen=50)\n","x_valid = pad_sequences(x_valid, maxlen=50)"],"metadata":{"id":"CJNVWntKjFug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OealShJDjH2l","executionInfo":{"status":"ok","timestamp":1681396725611,"user_tz":-540,"elapsed":3,"user":{"displayName":"강효은","userId":"09017213458312197996"}},"outputId":"f18e34cb-cee4-4710-ae48-89af26c67873"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[   0,    0,    0, ..., 2506, 3206,  447],\n","       [   0,    0,    0, ..., 4683,    6, 6358],\n","       [   0,    0,    0, ..., 1980,  733, 6359],\n","       ...,\n","       [   0,    0,    0, ...,    0,    8,  436],\n","       [   0,    0,    0, ..., 1139,    2,  307],\n","       [   0,    0,    0, ..., 1139,    2,  307]], dtype=int32)"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["y_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9Z667VQjW4F","executionInfo":{"status":"ok","timestamp":1681396727924,"user_tz":-540,"elapsed":7,"user":{"displayName":"강효은","userId":"09017213458312197996"}},"outputId":"016c3bba-0c71-4068-c873-126cc0e6bdd8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., ..., 0., 0., 0.],\n","       [1., 0., 0., ..., 0., 0., 0.],\n","       [1., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.],\n","       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen,vocab_size, embed_dim):\n","        super().__init__()\n","        self.t_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim) #텍스트 임베딩\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen=50\n","        print(\"t\")\n","        positions = tf.range(start=0, limit=maxlen, delta=1)\n","        positions = self.pos_emb(positions)\n","\n","        x = self.t_emb(x) #텍스트 임베딩\n","        return x + positions\n","class TransformerBlock(layers.Layer):  #공통적인 트랜스포머 블럭\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.att = MultiHeadAttention(embedding_dim=embed_dim, num_heads=num_heads)\n","        self.ffn = keras.Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs, training):\n","        attn_output = self.att(inputs)\n","        #print('어텐션 후',attn_output.shape)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        #print('드롭아웃',attn_output.shape)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        #print('레이어놈',out1.shape)\n","        ffn_output = self.ffn(out1)\n","        #print('ffn후',ffn_output.shape)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        print(\"트랜스포머\")\n","        return self.layernorm2(out1 + ffn_output)"],"metadata":{"id":"JQN4Tc-MjX3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(tf.keras.layers.Layer): #공통적인 트랜스포머 멀티헤드어텐션\n","    def __init__(self, embedding_dim, num_heads=4):\n","        super(MultiHeadAttention, self).__init__()\n","        self.embedding_dim = embedding_dim # d_model\n","        self.num_heads = num_heads\n","\n","        assert embedding_dim % self.num_heads == 0\n","\n","        self.projection_dim = embedding_dim // num_heads\n","        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n","        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n","        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n","        self.dense = tf.keras.layers.Dense(embedding_dim)\n","\n","    def scaled_dot_product_attention(self, query, key, value):\n","        matmul_qk = tf.matmul(query, key, transpose_b=True)\n","        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n","        logits = matmul_qk / tf.math.sqrt(depth)\n","        attention_weights = tf.nn.softmax(logits, axis=-1)\n","        output = tf.matmul(attention_weights, value)\n","        return output, attention_weights\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, inputs):\n","        # x.shape = [batch_size, seq_len, embedding_dim]\n","        batch_size = tf.shape(inputs)[0]\n","\n","        # (batch_size, seq_len, embedding_dim)\n","        query = self.query_dense(inputs)\n","        key = self.key_dense(inputs)\n","        value = self.value_dense(inputs)\n","\n","        # (batch_size, num_heads, seq_len, projection_dim)\n","        query = self.split_heads(query, batch_size)  \n","        key = self.split_heads(key, batch_size)\n","        value = self.split_heads(value, batch_size)\n","\n","        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n","        # (batch_size, seq_len, num_heads, projection_dim)\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n","\n","        # (batch_size, seq_len, embedding_dim)\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n","        outputs = self.dense(concat_attention)\n","        return outputs"],"metadata":{"id":"deEEvHWNjiCY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras import backend as K\n","def recall(y_target, y_pred):\n","    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n","    # round : 반올림한다\n","    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n","    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n","\n","    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n","    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n","\n","    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n","    count_true_positive_false_negative = K.sum(y_target_yn)\n","\n","    # Recall =  (True Positive) / (True Positive + False Negative)\n","    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n","    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n","\n","    # return a single tensor value\n","    return recall\n","\n","\n","def precision(y_target, y_pred):\n","    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n","    # round : 반올림한다\n","    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n","    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n","\n","    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n","    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n","\n","    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n","    count_true_positive_false_positive = K.sum(y_pred_yn)\n","\n","    # Precision = (True Positive) / (True Positive + False Positive)\n","    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n","    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n","\n","    # return a single tensor value\n","    return precision\n","\n","\n","def f1score(y_target, y_pred):\n","    _recall = recall(y_target, y_pred)\n","    _precision = precision(y_target, y_pred)\n","    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n","    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n","    \n","    # return a single tensor value\n","    return _f1score"],"metadata":{"id":"h7tUK-FijlWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embed_dim = 32 # Embedding size for each token\n","num_heads = 4  # Number of attention heads\n","ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n","maxlen=50\n","training=1\n","\n","text_input = layers.Input(shape=(50,)) #\n","t_embedding_layer = TokenAndPositionEmbedding(50,12440+1, embed_dim)\n","text_embedding = t_embedding_layer(text_input)  ########## \n","\n","transformer_block = TransformerBlock(32, num_heads, ff_dim) #일단은 블럭 하나만(추후 블럭 여러개 쌓아서 실험도 해보기!)\n","\n","output = transformer_block(text_embedding)\n","output = transformer_block(output) #트랜스포머 블럭 1층 추가\n","\n","x = layers.GlobalAveragePooling1D()(output)\n","x = layers.Dropout(0.1)(x)\n","x = layers.Dense(20, activation=\"relu\")(x)\n","x = layers.Dropout(0.1)(x)\n","\n","outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n","print(outputs.shape)\n","\n","model = keras.Model(inputs=text_input, outputs=outputs)\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', precision, recall, f1score])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EfzRpCLpjpq5","executionInfo":{"status":"ok","timestamp":1681397354071,"user_tz":-540,"elapsed":1946,"user":{"displayName":"강효은","userId":"09017213458312197996"}},"outputId":"1d110d2e-c1bc-4a3b-f88a-ee6efe229943"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["t\n","트랜스포머\n","트랜스포머\n","(None, 7)\n"]}]},{"cell_type":"code","source":["history = model.fit(\n","    x_train, y_train, batch_size=16, epochs=5, validation_data=(x_valid, y_valid)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDxCBuF4DomW","executionInfo":{"status":"ok","timestamp":1681397610492,"user_tz":-540,"elapsed":254488,"user":{"displayName":"강효은","userId":"09017213458312197996"}},"outputId":"e646df84-c30c-4e04-a0a4-3a44918f73bb"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","t\n","트랜스포머\n","트랜스포머\n","t\n","트랜스포머\n","트랜스포머\n","1531/1532 [============================>.] - ETA: 0s - loss: 0.6653 - accuracy: 0.7622 - precision: 0.8226 - recall: 0.6786 - f1score: 0.7248t\n","트랜스포머\n","트랜스포머\n","1532/1532 [==============================] - 60s 34ms/step - loss: 0.6653 - accuracy: 0.7622 - precision: 0.8227 - recall: 0.6787 - f1score: 0.7249 - val_loss: 0.8234 - val_accuracy: 0.7696 - val_precision: 0.7833 - val_recall: 0.7437 - val_f1score: 0.7624\n","Epoch 2/5\n","1532/1532 [==============================] - 49s 32ms/step - loss: 0.2199 - accuracy: 0.9324 - precision: 0.9402 - recall: 0.9242 - f1score: 0.9319 - val_loss: 0.7733 - val_accuracy: 0.7770 - val_precision: 0.7857 - val_recall: 0.7672 - val_f1score: 0.7760\n","Epoch 3/5\n","1532/1532 [==============================] - 48s 31ms/step - loss: 0.1363 - accuracy: 0.9590 - precision: 0.9640 - recall: 0.9546 - f1score: 0.9591 - val_loss: 1.0025 - val_accuracy: 0.7489 - val_precision: 0.7593 - val_recall: 0.7393 - val_f1score: 0.7489\n","Epoch 4/5\n","1532/1532 [==============================] - 46s 30ms/step - loss: 0.0879 - accuracy: 0.9739 - precision: 0.9769 - recall: 0.9718 - f1score: 0.9743 - val_loss: 1.0477 - val_accuracy: 0.7253 - val_precision: 0.7305 - val_recall: 0.7154 - val_f1score: 0.7226\n","Epoch 5/5\n","1532/1532 [==============================] - 50s 33ms/step - loss: 0.0731 - accuracy: 0.9782 - precision: 0.9806 - recall: 0.9756 - f1score: 0.9781 - val_loss: 1.2182 - val_accuracy: 0.6928 - val_precision: 0.6964 - val_recall: 0.6897 - val_f1score: 0.6929\n"]}]},{"cell_type":"code","source":["_loss, _acc, _precision, _recall, _f1score = model.evaluate(x_test, y_test, batch_size=16, verbose=1)\n","print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681397643278,"user_tz":-540,"elapsed":1715,"user":{"displayName":"강효은","userId":"09017213458312197996"}},"outputId":"a72bd5a4-3c3e-44aa-e154-baaaf457f2be","id":"WczB7oYQDu1l"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["164/164 [==============================] - 1s 8ms/step - loss: 1.4396 - accuracy: 0.6492 - precision: 0.6526 - recall: 0.6409 - f1score: 0.6465\n","loss: 1.440, accuracy: 0.649, precision: 0.653, recall: 0.641, f1score: 0.647\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PCuVrXp3G9AL"},"execution_count":null,"outputs":[]}]}