{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cf7953",
   "metadata": {},
   "source": [
    "# 여기서부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6b8984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_x</th>\n",
       "      <th>temp+eda</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>text_tokenize</th>\n",
       "      <th>Temp</th>\n",
       "      <th>EDA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.36212430820312924, 0.5950855218071341, 0.6...</td>\n",
       "      <td>[8, 5, 7, 276, 35, 2, 13, 723, 22, 280, 13, 84...</td>\n",
       "      <td>[34.81, 34.81, 34.81, 34.79, 34.79, 34.79, 34....</td>\n",
       "      <td>[2.856493, 2.788578, 2.678377, 2.652749, 2.645...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.09967221317831643, 0.46074755225157077, 0....</td>\n",
       "      <td>[137, 253, 31, 1042, 34, 845, 104, 1166, 979, ...</td>\n",
       "      <td>[34.79, 34.79, 34.79, 34.77, 34.77, 34.77, 34....</td>\n",
       "      <td>[2.647035, 2.653442, 2.631658, 2.614999, 2.623...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.5217207684902612, 0.7059450474628848, 0.63...</td>\n",
       "      <td>[7, 44, 99, 17, 49, 749, 82, 228, 6, 3, 2976, ...</td>\n",
       "      <td>[34.75, 34.75, 34.75, 34.75, 34.79, 34.79, 34....</td>\n",
       "      <td>[2.896217, 2.975664, 3.02692, 3.071769, 3.0922...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.013638213363584173, 0.4977522798008912, 0....</td>\n",
       "      <td>[99, 93, 35, 61, 262, 1, 723, 2179, 25, 2517, ...</td>\n",
       "      <td>[34.77, 34.77, 34.77, 34.77, 34.77, 34.77, 34....</td>\n",
       "      <td>[3.689019, 3.692863, 3.659546, 3.409672, 3.123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.1814917031611868, 0.5011223135638304, 0.14...</td>\n",
       "      <td>[15, 79, 16, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[34.55, 34.55, 34.55, 34.55, 34.55, 34.55, 34....</td>\n",
       "      <td>[4.412055, 4.536246, 4.630991, 4.669401, 4.661...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12688</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.0, 0.44890249617394556, 0.4151811098070448...</td>\n",
       "      <td>[96, 11, 65, 42, 258, 14989, 2, 240, 684, 2, 2...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.05, 35.05, 35....</td>\n",
       "      <td>[1.194537, 1.171473, 1.150971, 1.129189, 1.106...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12689</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.2285113780573238, 0.779642897290782, 0.457...</td>\n",
       "      <td>[3, 504, 494, 3, 4030, 800, 298, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[35.05, 35.05, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[0.929301, 0.922894, 0.917769, 0.916488, 0.912...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12690</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.0, 0.3924988321574137, 0.6015662496016466,...</td>\n",
       "      <td>[14, 65, 58, 14990, 164, 33, 1346, 2438, 11, 2...</td>\n",
       "      <td>[35.05, 35.05, 35.05, 35.05, 35.05, 35.05, 35....</td>\n",
       "      <td>[0.888298, 0.88061, 0.878048, 0.875485, 0.8742...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12691</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.0, 0.404710343900125, 0.6941450242437002, ...</td>\n",
       "      <td>[307, 20, 1533, 17, 1432, 63, 27, 1, 74, 47, 0...</td>\n",
       "      <td>[35.05, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[0.83192, 0.825513, 0.821669, 0.82295, 0.81910...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12692</th>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[[0.07021882960247872, 0.2635582968035488, 0.6...</td>\n",
       "      <td>[94, 20, 628, 12, 6064, 51, 219, 2990, 374, 0,...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[0.797324, 0.794761, 0.796043, 0.794761, 0.792...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12693 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment_x                                           temp+eda  \\\n",
       "0                0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "1                0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "2                0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "3                0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "4                0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "...            ...                                                ...   \n",
       "12688            0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12689            0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12690            0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12691            0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12692            0  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "\n",
       "                                                    mfcc  \\\n",
       "0      [[0.36212430820312924, 0.5950855218071341, 0.6...   \n",
       "1      [[0.09967221317831643, 0.46074755225157077, 0....   \n",
       "2      [[0.5217207684902612, 0.7059450474628848, 0.63...   \n",
       "3      [[0.013638213363584173, 0.4977522798008912, 0....   \n",
       "4      [[0.1814917031611868, 0.5011223135638304, 0.14...   \n",
       "...                                                  ...   \n",
       "12688  [[0.0, 0.44890249617394556, 0.4151811098070448...   \n",
       "12689  [[0.2285113780573238, 0.779642897290782, 0.457...   \n",
       "12690  [[0.0, 0.3924988321574137, 0.6015662496016466,...   \n",
       "12691  [[0.0, 0.404710343900125, 0.6941450242437002, ...   \n",
       "12692  [[0.07021882960247872, 0.2635582968035488, 0.6...   \n",
       "\n",
       "                                           text_tokenize  \\\n",
       "0      [8, 5, 7, 276, 35, 2, 13, 723, 22, 280, 13, 84...   \n",
       "1      [137, 253, 31, 1042, 34, 845, 104, 1166, 979, ...   \n",
       "2      [7, 44, 99, 17, 49, 749, 82, 228, 6, 3, 2976, ...   \n",
       "3      [99, 93, 35, 61, 262, 1, 723, 2179, 25, 2517, ...   \n",
       "4      [15, 79, 16, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "...                                                  ...   \n",
       "12688  [96, 11, 65, 42, 258, 14989, 2, 240, 684, 2, 2...   \n",
       "12689  [3, 504, 494, 3, 4030, 800, 298, 0, 0, 0, 0, 0...   \n",
       "12690  [14, 65, 58, 14990, 164, 33, 1346, 2438, 11, 2...   \n",
       "12691  [307, 20, 1533, 17, 1432, 63, 27, 1, 74, 47, 0...   \n",
       "12692  [94, 20, 628, 12, 6064, 51, 219, 2990, 374, 0,...   \n",
       "\n",
       "                                                    Temp  \\\n",
       "0      [34.81, 34.81, 34.81, 34.79, 34.79, 34.79, 34....   \n",
       "1      [34.79, 34.79, 34.79, 34.77, 34.77, 34.77, 34....   \n",
       "2      [34.75, 34.75, 34.75, 34.75, 34.79, 34.79, 34....   \n",
       "3      [34.77, 34.77, 34.77, 34.77, 34.77, 34.77, 34....   \n",
       "4      [34.55, 34.55, 34.55, 34.55, 34.55, 34.55, 34....   \n",
       "...                                                  ...   \n",
       "12688  [35.07, 35.07, 35.07, 35.07, 35.05, 35.05, 35....   \n",
       "12689  [35.05, 35.05, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12690  [35.05, 35.05, 35.05, 35.05, 35.05, 35.05, 35....   \n",
       "12691  [35.05, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12692  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "\n",
       "                                                     EDA  \n",
       "0      [2.856493, 2.788578, 2.678377, 2.652749, 2.645...  \n",
       "1      [2.647035, 2.653442, 2.631658, 2.614999, 2.623...  \n",
       "2      [2.896217, 2.975664, 3.02692, 3.071769, 3.0922...  \n",
       "3      [3.689019, 3.692863, 3.659546, 3.409672, 3.123...  \n",
       "4      [4.412055, 4.536246, 4.630991, 4.669401, 4.661...  \n",
       "...                                                  ...  \n",
       "12688  [1.194537, 1.171473, 1.150971, 1.129189, 1.106...  \n",
       "12689  [0.929301, 0.922894, 0.917769, 0.916488, 0.912...  \n",
       "12690  [0.888298, 0.88061, 0.878048, 0.875485, 0.8742...  \n",
       "12691  [0.83192, 0.825513, 0.821669, 0.82295, 0.81910...  \n",
       "12692  [0.797324, 0.794761, 0.796043, 0.794761, 0.792...  \n",
       "\n",
       "[12693 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('0404_result_data.pkl',\"rb\") as fr:\n",
    "    data = pickle.load(fr)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c6f755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import nn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f9eb7c9",
   "metadata": {},
   "source": [
    "# train에다 적절하게 원하는 도메인의 데이터 넣어주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd080b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp+eda</th>\n",
       "      <th>text_tokenize</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>Temp</th>\n",
       "      <th>EDA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[3, 285, 298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.44487094436175745, 0.7782266401639272, 0.7...</td>\n",
       "      <td>[35.79, 35.81, 35.81, 35.81, 35.81, 35.81, 35....</td>\n",
       "      <td>[1.170361, 1.171642, 1.179331, 1.172924, 1.156...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[37, 17, 301, 20, 2, 13, 99, 2, 13, 10409, 372...</td>\n",
       "      <td>[[0.01151864290696547, 0.4609377509063598, 0.7...</td>\n",
       "      <td>[33.73, 33.73, 33.75, 33.75, 33.75, 33.75, 33....</td>\n",
       "      <td>[9.616379, 9.558715, 9.483112, 9.463891, 9.513...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[45, 21, 2249, 4849, 3733, 11, 74, 19, 684, 10...</td>\n",
       "      <td>[[0.0, 0.4195318523402386, 0.581080516368257, ...</td>\n",
       "      <td>[33.93, 33.93, 33.91, 33.91, 33.91, 33.91, 33....</td>\n",
       "      <td>[15.386588, 15.355834, 15.191814, 15.148247, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[14, 53, 2, 1092, 1407, 50, 38, 24, 0, 0, 0, 0...</td>\n",
       "      <td>[[0.0, 0.5025080177790993, 0.7498997350502701,...</td>\n",
       "      <td>[34.07, 34.07, 34.07, 34.07, 34.05, 34.05, 34....</td>\n",
       "      <td>[10.503923, 10.498798, 10.51033, 10.511612, 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[15, 154, 147, 202, 22, 292, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.0, 0.5547598822487174, 0.5632170254887404,...</td>\n",
       "      <td>[33.63, 33.61, 33.61, 33.61, 33.61, 33.61, 33....</td>\n",
       "      <td>[11.218114, 11.186079, 11.089973, 10.973365, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[15, 114, 1298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.3874501701301818, 0.5760761224757045, 0.54...</td>\n",
       "      <td>[34.09, 34.09, 34.09, 34.09, 34.09, 34.09, 34....</td>\n",
       "      <td>[7.837373, 7.888633, 7.964241, 8.039848, 8.091...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6896</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[60, 8, 5, 9, 2, 6284, 158, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.19742854306196056, 0.5854426585818188, 0.7...</td>\n",
       "      <td>[31.77, 31.77, 31.77, 31.77, 31.79, 31.79, 31....</td>\n",
       "      <td>[2.77543, 2.777992, 2.783118, 2.783118, 2.7882...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4063</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[87, 87, 7, 716, 3392, 7, 45, 2401, 77, 1, 295...</td>\n",
       "      <td>[[0.08914083718825794, 0.4618189193481648, 0.7...</td>\n",
       "      <td>[32.55, 32.55, 32.55, 32.57, 32.57, 32.57, 32....</td>\n",
       "      <td>[2.540382, 2.541663, 2.5391, 2.5391, 2.537819,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3481</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[166, 3, 243, 926, 2719, 28, 1497, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.3604220617484546, 0.7114072268831756, 0.36...</td>\n",
       "      <td>[32.0, 32.0, 31.99, 31.99, 31.99, 31.99, 31.99...</td>\n",
       "      <td>[18.746672, 18.867109, 18.920921, 18.864546, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9771</th>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[34, 587, 17, 126, 68, 360, 303, 15, 67, 13316...</td>\n",
       "      <td>[[0.007221525929705708, 0.4736806191579831, 0....</td>\n",
       "      <td>[29.87, 29.87, 29.87, 29.87, 29.87, 29.87, 29....</td>\n",
       "      <td>[0.165302, 0.189648, 0.167865, 0.167865, 0.167...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1269 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               temp+eda  \\\n",
       "388   [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "5361  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "808   [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "6096  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "1578  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "...                                                 ...   \n",
       "2083  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "6896  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "4063  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "3481  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "9771  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "\n",
       "                                          text_tokenize  \\\n",
       "388   [3, 285, 298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "5361  [37, 17, 301, 20, 2, 13, 99, 2, 13, 10409, 372...   \n",
       "808   [45, 21, 2249, 4849, 3733, 11, 74, 19, 684, 10...   \n",
       "6096  [14, 53, 2, 1092, 1407, 50, 38, 24, 0, 0, 0, 0...   \n",
       "1578  [15, 154, 147, 202, 22, 292, 0, 0, 0, 0, 0, 0,...   \n",
       "...                                                 ...   \n",
       "2083  [15, 114, 1298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "6896  [60, 8, 5, 9, 2, 6284, 158, 0, 0, 0, 0, 0, 0, ...   \n",
       "4063  [87, 87, 7, 716, 3392, 7, 45, 2401, 77, 1, 295...   \n",
       "3481  [166, 3, 243, 926, 2719, 28, 1497, 0, 0, 0, 0,...   \n",
       "9771  [34, 587, 17, 126, 68, 360, 303, 15, 67, 13316...   \n",
       "\n",
       "                                                   mfcc  \\\n",
       "388   [[0.44487094436175745, 0.7782266401639272, 0.7...   \n",
       "5361  [[0.01151864290696547, 0.4609377509063598, 0.7...   \n",
       "808   [[0.0, 0.4195318523402386, 0.581080516368257, ...   \n",
       "6096  [[0.0, 0.5025080177790993, 0.7498997350502701,...   \n",
       "1578  [[0.0, 0.5547598822487174, 0.5632170254887404,...   \n",
       "...                                                 ...   \n",
       "2083  [[0.3874501701301818, 0.5760761224757045, 0.54...   \n",
       "6896  [[0.19742854306196056, 0.5854426585818188, 0.7...   \n",
       "4063  [[0.08914083718825794, 0.4618189193481648, 0.7...   \n",
       "3481  [[0.3604220617484546, 0.7114072268831756, 0.36...   \n",
       "9771  [[0.007221525929705708, 0.4736806191579831, 0....   \n",
       "\n",
       "                                                   Temp  \\\n",
       "388   [35.79, 35.81, 35.81, 35.81, 35.81, 35.81, 35....   \n",
       "5361  [33.73, 33.73, 33.75, 33.75, 33.75, 33.75, 33....   \n",
       "808   [33.93, 33.93, 33.91, 33.91, 33.91, 33.91, 33....   \n",
       "6096  [34.07, 34.07, 34.07, 34.07, 34.05, 34.05, 34....   \n",
       "1578  [33.63, 33.61, 33.61, 33.61, 33.61, 33.61, 33....   \n",
       "...                                                 ...   \n",
       "2083  [34.09, 34.09, 34.09, 34.09, 34.09, 34.09, 34....   \n",
       "6896  [31.77, 31.77, 31.77, 31.77, 31.79, 31.79, 31....   \n",
       "4063  [32.55, 32.55, 32.55, 32.57, 32.57, 32.57, 32....   \n",
       "3481  [32.0, 32.0, 31.99, 31.99, 31.99, 31.99, 31.99...   \n",
       "9771  [29.87, 29.87, 29.87, 29.87, 29.87, 29.87, 29....   \n",
       "\n",
       "                                                    EDA  \n",
       "388   [1.170361, 1.171642, 1.179331, 1.172924, 1.156...  \n",
       "5361  [9.616379, 9.558715, 9.483112, 9.463891, 9.513...  \n",
       "808   [15.386588, 15.355834, 15.191814, 15.148247, 1...  \n",
       "6096  [10.503923, 10.498798, 10.51033, 10.511612, 10...  \n",
       "1578  [11.218114, 11.186079, 11.089973, 10.973365, 1...  \n",
       "...                                                 ...  \n",
       "2083  [7.837373, 7.888633, 7.964241, 8.039848, 8.091...  \n",
       "6896  [2.77543, 2.777992, 2.783118, 2.783118, 2.7882...  \n",
       "4063  [2.540382, 2.541663, 2.5391, 2.5391, 2.537819,...  \n",
       "3481  [18.746672, 18.867109, 18.920921, 18.864546, 1...  \n",
       "9771  [0.165302, 0.189648, 0.167865, 0.167865, 0.167...  \n",
       "\n",
       "[1269 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(data[['temp+eda','text_tokenize','mfcc','Temp','EDA']], data['sentiment_x'], test_size=0.2, shuffle=True, random_state=4)\n",
    "x_valid,x_text, y_valid, y_test = train_test_split(x_valid, y_valid, test_size=0.5, shuffle=True, random_state=4)\n",
    "x_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc484da4",
   "metadata": {},
   "source": [
    "# audio, text 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c40d12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class a_TokenAndPositionEmbedding(layers.Layer):#오디오 임베딩\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.a_emb = tf.keras.layers.Conv1D(filters=32, kernel_size=551,padding='valid', activation='relu',input_shape=(600,50)) #(1,50,32)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) #(50,32)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 50\n",
    "        \n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.a_emb(tf.convert_to_tensor(x)) #self.a_emb(np.array([x]))[0] #오디오 임베딩\n",
    "        \n",
    "        return x + positions\n",
    "\n",
    "\n",
    "\n",
    "class t_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen,vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.t_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim) #텍스트 임베딩\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen=50\n",
    "        print(\"t\")\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.t_emb(x) #텍스트 임베딩\n",
    "        return x + positions\n",
    "    \n",
    "class bio_TokenAndPositionEmbedding(layers.Layer): #바이오 멀티모달 임베딩(temp, eda concat한거)\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bio_temp_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #토큰 \n",
    "        self.bio_eda_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #토큰 임베딩\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 160\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        y1 = self.bio_temp_emb(tf.slice(x,[0,0],[-1,80])) #토큰임베딩 진행\n",
    "        y2= self.bio_eda_emb(tf.slice(x,[0,80],[-1,-1]))\n",
    "\n",
    "        x=tf.concat([y1,y2],axis=1)\n",
    "        \n",
    "\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb043a85",
   "metadata": {},
   "source": [
    "# a->t 트랜스포머 (쿼리가 텍스트, 키 밸류는 오디오), 타겟:텍스트, 소스: 오디오\n",
    "\n",
    "텍스트, 오디오 순서 인풋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38be2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class at_MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(at_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio): ##################################################33\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_text)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_text)\n",
    "        key = self.key_dense(inputs_audio)\n",
    "        value = self.value_dense(inputs_audio)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class at_co_TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = at_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio, training):\n",
    "        attn_output = self.att(inputs_text, inputs_audio)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_text + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0c0c668",
   "metadata": {},
   "source": [
    "# ta_트랜스포머  : (쿼리가 오디오, 키 밸류는 텍스트), 타겟:오디오, 소스: 텍스트\n",
    "오디오 텍스트 순서 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d670886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ta_MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(ta_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text): ##################################################33\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_audio)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_audio)\n",
    "        key = self.key_dense(inputs_text)\n",
    "        value = self.value_dense(inputs_text)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class ta_co_TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = ta_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text, training):\n",
    "        attn_output = self.att(inputs_audio, inputs_text)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_audio + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        print(\"text->au 트랜스포머\")\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47c0c1f1",
   "metadata": {},
   "source": [
    "# 트랜스포머\n",
    "# 바이오는 co-attetnion이 아니고 그냥 concat해서 input되기때문에 그냥 트랜스포머 block을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f0b7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):  #공통적인 트랜스포머 블럭\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(embedding_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        #print('어텐션 후',attn_output.shape)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        #print('드롭아웃',attn_output.shape)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        #print('레이어놈',out1.shape)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        #print('ffn후',ffn_output.shape)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        print(\"트랜스포머\")\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c72151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer): #공통적인 트랜스포머 멀티헤드어텐션\n",
    "    def __init__(self, embedding_dim, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "789aceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical #원핫인코딩\n",
    "y_valid = to_categorical(y_valid)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b59b6eca",
   "metadata": {},
   "source": [
    "# recall, f1score, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71e72d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import backend as K\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd7688da",
   "metadata": {},
   "source": [
    "# 멀티모달 트랜스포머"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "159e4f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio (None, 600, 50)\n",
      "t\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n",
      "after trans (None, 50, 32) (None, 50, 32) (None, 160, 32)\n",
      "after pooling (None, 32) (None, 32) (None, 32)\n",
      "concat (None, 96)\n",
      "트랜스포머\n",
      "(None, 7)\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 600, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "t__token_and_position_embedding (None, 50, 32)       481312      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "a__token_and_position_embedding (None, 50, 32)       883232      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bio__token_and_position_embeddi (None, 160, 32)      10240       input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "at_co__transformer_block_1 (at_ (None, 50, 32)       6464        t__token_and_position_embedding_1\n",
      "                                                                 a__token_and_position_embedding_1\n",
      "__________________________________________________________________________________________________\n",
      "ta_co__transformer_block_1 (ta_ (None, 50, 32)       6464        a__token_and_position_embedding_1\n",
      "                                                                 t__token_and_position_embedding_1\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_2 (Transforme (None, 160, 32)      6464        bio__token_and_position_embedding\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           at_co__transformer_block_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           ta_co__transformer_block_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 32)           0           transformer_block_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_3 (Transforme (None, None, 96)     43904       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 96)           0           transformer_block_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 96)           0           global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 20)           1940        dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 20)           0           dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 7)            147         dropout_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,440,167\n",
      "Trainable params: 1,440,167\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32 # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=160\n",
    "training=1\n",
    "\n",
    "audio_input = layers.Input(shape=(600,50)) #\n",
    "text_input = layers.Input(shape=(50,)) #\n",
    "bio_input = layers.Input(shape=(160,)) #\n",
    "\n",
    "#input_audio text bio 는 따로 정해줘야함\n",
    "\n",
    "a_embedding_layer = a_TokenAndPositionEmbedding(50, 32) \n",
    "print(\"audio\",audio_input.shape)\n",
    "audio_embedding = a_embedding_layer(audio_input)######### ################################################################3\n",
    "\n",
    "t_embedding_layer = t_TokenAndPositionEmbedding(50,14991, embed_dim)\n",
    "text_embedding = t_embedding_layer(text_input)  ########## \n",
    "\n",
    "b_embedding_layer = bio_TokenAndPositionEmbedding(160, embed_dim)\n",
    "bio_embedding = b_embedding_layer(tf.convert_to_tensor(bio_input))  ########## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 멀티모달 co-attention\n",
    "at_transformer_block = at_co_TransformerBlock(32, 4, 32)\n",
    "ta_transformer_block = ta_co_TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "bio_transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block = TransformerBlock(96, num_heads, ff_dim)\n",
    "\n",
    "\n",
    "#진행되는 부분\n",
    "output_at = at_transformer_block(text_embedding, audio_embedding) ############ (50,50,32)\n",
    "output_ta = ta_transformer_block(audio_embedding, text_embedding) # (50,50,32)\n",
    "output_bio = bio_transformer_block(bio_embedding) # (160,160,32)\n",
    "print(\"after trans\", output_at.shape, output_ta.shape, output_bio.shape)\n",
    "output_at = layers.GlobalAveragePooling1D()(output_at)\n",
    "output_ta = layers.GlobalAveragePooling1D()(output_ta)\n",
    "output_bio = layers.GlobalAveragePooling1D()(output_bio)\n",
    "print(\"after pooling\", output_at.shape, output_ta.shape, output_bio.shape)\n",
    "concat_embedding = layers.concatenate([output_at, output_ta, output_bio])#tf.concat([output_at, output_ta, output_bio], axis=0) #concat 진행\n",
    "print(\"concat\", concat_embedding.shape)\n",
    "output = transformer_block(concat_embedding)  ############ concat 후 트랜스포머 블럭\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(output)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n",
    "print(outputs.shape)\n",
    "\n",
    "model = keras.Model(inputs=[audio_input, text_input, bio_input], outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', precision, recall, f1score])\n",
    "model.summary()\n",
    "\n",
    "#멀티인풋->하나의 아웃풋이어야함\n",
    "#트랜스포머 블럭별로 하나의 Model 만들어서 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcd3b0c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "635/635 [==============================] - 77s 121ms/step - loss: 0.5660 - accuracy: 0.8295 - precision: 0.8826 - recall: 0.7829 - f1score: 0.8277 - val_loss: 0.5604 - val_accuracy: 0.8239 - val_precision: 0.8956 - val_recall: 0.7554 - val_f1score: 0.8172\n",
      "Epoch 2/5\n",
      "635/635 [==============================] - 81s 128ms/step - loss: 0.5277 - accuracy: 0.8432 - precision: 0.9020 - recall: 0.7891 - f1score: 0.8396 - val_loss: 0.5470 - val_accuracy: 0.8338 - val_precision: 0.8693 - val_recall: 0.8014 - val_f1score: 0.8329\n",
      "Epoch 3/5\n",
      "635/635 [==============================] - 77s 122ms/step - loss: 0.4886 - accuracy: 0.8591 - precision: 0.9039 - recall: 0.8195 - f1score: 0.8582 - val_loss: 0.5665 - val_accuracy: 0.8342 - val_precision: 0.8587 - val_recall: 0.8116 - val_f1score: 0.8337\n",
      "Epoch 4/5\n",
      "635/635 [==============================] - 80s 126ms/step - loss: 0.4605 - accuracy: 0.8641 - precision: 0.9098 - recall: 0.8319 - f1score: 0.8678 - val_loss: 0.5876 - val_accuracy: 0.8354 - val_precision: 0.8571 - val_recall: 0.8110 - val_f1score: 0.8326\n",
      "Epoch 5/5\n",
      "635/635 [==============================] - 77s 122ms/step - loss: 0.4347 - accuracy: 0.8735 - precision: 0.9145 - recall: 0.8474 - f1score: 0.8786 - val_loss: 0.6281 - val_accuracy: 0.8338 - val_precision: 0.8550 - val_recall: 0.8025 - val_f1score: 0.8270\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [[np.array(x_train['mfcc'].to_list()),np.array(x_train['text_tokenize'].to_list()),np.array(x_train['temp+eda'].to_list())]], y_train, batch_size=16, epochs=5, validation_data=([[np.array(x_valid['mfcc'].to_list()),np.array(x_valid['text_tokenize'].to_list()),np.array(x_valid['temp+eda'].to_list())]], y_valid)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9931e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 5s 32ms/step - loss: 0.6281 - accuracy: 0.8338 - precision: 0.8550 - recall: 0.8025 - f1score: 0.8270\n",
      "loss: 0.628, accuracy: 0.834, precision: 0.855, recall: 0.803, f1score: 0.827\n"
     ]
    }
   ],
   "source": [
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate([[np.array(x_valid['mfcc'].to_list()),np.array(x_valid['text_tokenize'].to_list()),np.array(x_valid['temp+eda'].to_list())]], y_valid, batch_size=16, verbose=1)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efd4981b",
   "metadata": {},
   "source": [
    "# 텍스트 트랜스포머\n",
    "텍스트 maxlen은 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fef07f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "트랜스포머\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32 # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=50\n",
    "training=1\n",
    "\n",
    "text_input = layers.Input(shape=(50,)) #\n",
    "t_embedding_layer = t_TokenAndPositionEmbedding(50,14991, embed_dim)\n",
    "text_embedding = t_embedding_layer(text_input)  ########## \n",
    "\n",
    "transformer_block = TransformerBlock(32, num_heads, ff_dim) #일단은 블럭 하나만(추후 블럭 여러개 쌓아서 실험도 해보기!)\n",
    "\n",
    "output = transformer_block(text_embedding)\n",
    "\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(output)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n",
    "print(outputs.shape)\n",
    "\n",
    "model = keras.Model(inputs=text_input, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1e72b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "t\n",
      "트랜스포머\n",
      "t\n",
      "트랜스포머\n",
      "632/635 [============================>.] - ETA: 0s - loss: 0.6998 - accuracy: 0.8273t\n",
      "트랜스포머\n",
      "635/635 [==============================] - 15s 19ms/step - loss: 0.6996 - accuracy: 0.8273 - val_loss: 0.6466 - val_accuracy: 0.8251\n",
      "Epoch 2/5\n",
      "635/635 [==============================] - 11s 17ms/step - loss: 0.6174 - accuracy: 0.8301 - val_loss: 0.5920 - val_accuracy: 0.8258\n",
      "Epoch 3/5\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 0.4745 - accuracy: 0.8522 - val_loss: 0.6313 - val_accuracy: 0.8211\n",
      "Epoch 4/5\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 0.3060 - accuracy: 0.8980 - val_loss: 0.7406 - val_accuracy: 0.8006\n",
      "Epoch 5/5\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 0.1900 - accuracy: 0.9353 - val_loss: 0.8911 - val_accuracy: 0.8077\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [np.array(x_train['text_tokenize'].to_list())], y_train, batch_size=16, epochs=5, validation_data=([np.array(x_valid['text_tokenize'].to_list())], y_valid)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f3b467d8",
   "metadata": {},
   "source": [
    "# temp 트랜스포머\n",
    "temp maxlen은 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e33bd50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class temp_TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len, embed_dimm):\n",
    "        super(temp_TokenAndPositionEmbedding, self).__init__()\n",
    "        self.temp_emb = tf.keras.layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        max_len = 80\n",
    "        positions = tf.range(start=0, limit=max_len, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.temp_emb(x)\n",
    "        return x + positions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a4dfc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트랜스포머\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32 # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=50\n",
    "training=1\n",
    "\n",
    "temp_input = layers.Input(shape=(80,)) #\n",
    "temp_embedding_layer = temp_TokenAndPositionEmbedding(80, embed_dim)\n",
    "temp_embedding = temp_embedding_layer(temp_input)  ########## \n",
    "\n",
    "transformer_block = TransformerBlock(32, num_heads, ff_dim) #일단은 블럭 하나만(추후 블럭 여러개 쌓아서 실험도 해보기!)\n",
    "\n",
    "output = transformer_block(temp_embedding)\n",
    "\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(output)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n",
    "print(outputs.shape)\n",
    "\n",
    "model = keras.Model(inputs=temp_input, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a019faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "트랜스포머\n",
      "트랜스포머\n",
      "631/635 [============================>.] - ETA: 0s - loss: 0.7511 - accuracy: 0.8161트랜스포머\n",
      "635/635 [==============================] - 15s 19ms/step - loss: 0.7501 - accuracy: 0.8164 - val_loss: 0.6612 - val_accuracy: 0.8251\n",
      "Epoch 2/5\n",
      "635/635 [==============================] - 12s 19ms/step - loss: 0.6886 - accuracy: 0.8283 - val_loss: 0.6528 - val_accuracy: 0.8251\n",
      "Epoch 3/5\n",
      "635/635 [==============================] - 10s 16ms/step - loss: 0.6836 - accuracy: 0.8288 - val_loss: 0.6590 - val_accuracy: 0.8251\n",
      "Epoch 4/5\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 0.6760 - accuracy: 0.8288 - val_loss: 0.6500 - val_accuracy: 0.8251\n",
      "Epoch 5/5\n",
      "635/635 [==============================] - 13s 20ms/step - loss: 0.6703 - accuracy: 0.8288 - val_loss: 0.6562 - val_accuracy: 0.8251\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [np.array(x_train['Temp'].to_list())], y_train, batch_size=16, epochs=5, validation_data=([np.array(x_valid['Temp'].to_list())], y_valid)\n",
    ") #얜 왜이럴까"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f26fe",
   "metadata": {},
   "source": [
    "# 오디오 트랜스포머\n",
    "오디오 maxlen은 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35305095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트랜스포머\n",
      "(None, 7)\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32 # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=50\n",
    "training=1\n",
    "\n",
    "audio_input = layers.Input(shape=(600,50)) #\n",
    "\n",
    "\n",
    "#input_audio text bio 는 따로 정해줘야함\n",
    "\n",
    "a_embedding_layer = a_TokenAndPositionEmbedding(50, 32) \n",
    "\n",
    "audio_embedding = a_embedding_layer(audio_input)#\n",
    "\n",
    "transformer_block = TransformerBlock(32, num_heads, ff_dim) #일단은 블럭 하나만(추후 블럭 여러개 쌓아서 실험도 해보기!)\n",
    "\n",
    "output = transformer_block(audio_embedding )\n",
    "\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(output)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n",
    "print(outputs.shape)\n",
    "\n",
    "model = keras.Model(inputs=audio_input, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [np.array(x_train['mfcc'].to_list())], y_train, batch_size=16, epochs=5, validation_data=([np.array(x_valid['mfcc'].to_list())], y_valid)\n",
    ") #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d993a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mul_s",
   "language": "python",
   "name": "mul_s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
