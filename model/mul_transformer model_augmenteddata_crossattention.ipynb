{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cf7953",
   "metadata": {},
   "source": [
    "# 여기서부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afd7878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 20:50:00.000258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7412946311042373608\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10764100608\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 15659875815949708180\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10764100608\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 2866239180244179789\n",
      "physical_device_desc: \"device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 20:50:01.748305: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-13 20:50:01.749692: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-04-13 20:50:01.750499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-13 20:50:01.951747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:01.952183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-04-13 20:50:01.952268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:01.952643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-04-13 20:50:01.952666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-13 20:50:01.979469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-13 20:50:01.979563: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-04-13 20:50:01.994187: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-13 20:50:01.997071: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-13 20:50:02.021897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-13 20:50:02.025361: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-13 20:50:02.071936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-13 20:50:02.072096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:02.072554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:02.072937: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:02.073367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:02.073708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-04-13 20:50:02.074118: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-13 20:50:03.186725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-13 20:50:03.186748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2023-04-13 20:50:03.186752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2023-04-13 20:50:03.186755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2023-04-13 20:50:03.187337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:03.187721: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:03.188081: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:03.188399: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:03.188710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2023-04-13 20:50:03.190053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:03.190416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 20:50:03.190720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:1 with 10265 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6b8984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pickle\n",
    "# train\n",
    "with open(\"augmentedhalf1.pkl\",\"rb\") as fr:\n",
    "    data = pickle.load(fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef797fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid.pkl\",\"rb\") as fr:\n",
    "    valid = pickle.load(fr)\n",
    "valid=valid.drop(['Segment ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "136147c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.pkl\",\"rb\") as fr:\n",
    "    test = pickle.load(fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00e55ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment ID</th>\n",
       "      <th>sentiment_x</th>\n",
       "      <th>Temp</th>\n",
       "      <th>EDA</th>\n",
       "      <th>temp+eda</th>\n",
       "      <th>text_tokenize</th>\n",
       "      <th>mfcc_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>Sess08_script01_User015F_001</td>\n",
       "      <td>1</td>\n",
       "      <td>[33.07, 33.09, 33.09, 33.09, 33.09, 33.09, 33....</td>\n",
       "      <td>[3.239397, 3.234272, 3.229146, 3.243241, 3.245...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[8, 72, 14, 166, 733, 26, 21, 37, 733, 50, 16,...</td>\n",
       "      <td>[[0.0, 0.37972227268282244, 0.8198979194967001...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>Sess08_script01_User016F_001</td>\n",
       "      <td>0</td>\n",
       "      <td>[33.31, 33.31, 33.31, 33.31, 33.31, 33.31, 33....</td>\n",
       "      <td>[4.143951, 4.138826, 4.134982, 4.131138, 4.128...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[8, 488, 926, 50, 85, 3902, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0.0, 0.5751133741423142, 0.7879647360973706,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>Sess08_script01_User015F_002</td>\n",
       "      <td>0</td>\n",
       "      <td>[33.07, 33.07, 33.05, 33.05, 33.05, 33.05, 33....</td>\n",
       "      <td>[3.453379, 3.458504, 3.450816, 3.448254, 3.443...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[14, 11, 2367, 4, 786, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.3555334572333074, 0.6510807576689936, 0.80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>Sess08_script01_User016F_002</td>\n",
       "      <td>0</td>\n",
       "      <td>[33.27, 33.27, 33.25, 33.25, 33.25, 33.25, 33....</td>\n",
       "      <td>[4.079888, 4.072201, 4.045294, 4.041451, 4.023...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[87, 251, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.0, 0.5211673046270286, 0.5221022623447374,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>Sess08_script01_User015F_003</td>\n",
       "      <td>1</td>\n",
       "      <td>[33.03, 33.03, 33.03, 33.03, 33.03, 33.03, 33....</td>\n",
       "      <td>[3.438003, 3.445691, 3.480287, 3.523852, 3.548...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[3, 139, 1703, 104, 11, 583, 105, 72, 223, 119...</td>\n",
       "      <td>[[0.2308842054093153, 0.785980178750852, 0.827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12688</th>\n",
       "      <td>Sess40_script06_User079F_043</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.05, 35.05, 35....</td>\n",
       "      <td>[1.194537, 1.171473, 1.150971, 1.129189, 1.106...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[96, 11, 65, 42, 258, 14989, 2, 240, 684, 2, 2...</td>\n",
       "      <td>[[0.0, 0.44890249617394556, 0.4151811098070448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12689</th>\n",
       "      <td>Sess40_script06_User079F_044</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.05, 35.05, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[0.929301, 0.922894, 0.917769, 0.916488, 0.912...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[3, 504, 494, 3, 4030, 800, 298, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[0.2285113780573238, 0.779642897290782, 0.457...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12690</th>\n",
       "      <td>Sess40_script06_User079F_045</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.05, 35.05, 35.05, 35.05, 35.05, 35.05, 35....</td>\n",
       "      <td>[0.888298, 0.88061, 0.878048, 0.875485, 0.8742...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[14, 65, 58, 14990, 164, 33, 1346, 2438, 11, 2...</td>\n",
       "      <td>[[0.0, 0.3924988321574137, 0.6015662496016466,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12691</th>\n",
       "      <td>Sess40_script06_User079F_046</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.05, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[0.83192, 0.825513, 0.821669, 0.82295, 0.81910...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[307, 20, 1533, 17, 1432, 63, 27, 1, 74, 47, 0...</td>\n",
       "      <td>[[0.0, 0.404710343900125, 0.6941450242437002, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12692</th>\n",
       "      <td>Sess40_script06_User079F_047</td>\n",
       "      <td>0</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[0.797324, 0.794761, 0.796043, 0.794761, 0.792...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[94, 20, 628, 12, 6064, 51, 219, 2990, 374, 0,...</td>\n",
       "      <td>[[0.07021882960247872, 0.2635582968035488, 0.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2567 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Segment ID  sentiment_x  \\\n",
       "2230   Sess08_script01_User015F_001            1   \n",
       "2231   Sess08_script01_User016F_001            0   \n",
       "2232   Sess08_script01_User015F_002            0   \n",
       "2233   Sess08_script01_User016F_002            0   \n",
       "2234   Sess08_script01_User015F_003            1   \n",
       "...                             ...          ...   \n",
       "12688  Sess40_script06_User079F_043            0   \n",
       "12689  Sess40_script06_User079F_044            0   \n",
       "12690  Sess40_script06_User079F_045            0   \n",
       "12691  Sess40_script06_User079F_046            0   \n",
       "12692  Sess40_script06_User079F_047            0   \n",
       "\n",
       "                                                    Temp  \\\n",
       "2230   [33.07, 33.09, 33.09, 33.09, 33.09, 33.09, 33....   \n",
       "2231   [33.31, 33.31, 33.31, 33.31, 33.31, 33.31, 33....   \n",
       "2232   [33.07, 33.07, 33.05, 33.05, 33.05, 33.05, 33....   \n",
       "2233   [33.27, 33.27, 33.25, 33.25, 33.25, 33.25, 33....   \n",
       "2234   [33.03, 33.03, 33.03, 33.03, 33.03, 33.03, 33....   \n",
       "...                                                  ...   \n",
       "12688  [35.07, 35.07, 35.07, 35.07, 35.05, 35.05, 35....   \n",
       "12689  [35.05, 35.05, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12690  [35.05, 35.05, 35.05, 35.05, 35.05, 35.05, 35....   \n",
       "12691  [35.05, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12692  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "\n",
       "                                                     EDA  \\\n",
       "2230   [3.239397, 3.234272, 3.229146, 3.243241, 3.245...   \n",
       "2231   [4.143951, 4.138826, 4.134982, 4.131138, 4.128...   \n",
       "2232   [3.453379, 3.458504, 3.450816, 3.448254, 3.443...   \n",
       "2233   [4.079888, 4.072201, 4.045294, 4.041451, 4.023...   \n",
       "2234   [3.438003, 3.445691, 3.480287, 3.523852, 3.548...   \n",
       "...                                                  ...   \n",
       "12688  [1.194537, 1.171473, 1.150971, 1.129189, 1.106...   \n",
       "12689  [0.929301, 0.922894, 0.917769, 0.916488, 0.912...   \n",
       "12690  [0.888298, 0.88061, 0.878048, 0.875485, 0.8742...   \n",
       "12691  [0.83192, 0.825513, 0.821669, 0.82295, 0.81910...   \n",
       "12692  [0.797324, 0.794761, 0.796043, 0.794761, 0.792...   \n",
       "\n",
       "                                                temp+eda  \\\n",
       "2230   [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "2231   [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "2232   [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "2233   [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "2234   [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "...                                                  ...   \n",
       "12688  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12689  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12690  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12691  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "12692  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "\n",
       "                                           text_tokenize  \\\n",
       "2230   [8, 72, 14, 166, 733, 26, 21, 37, 733, 50, 16,...   \n",
       "2231   [8, 488, 926, 50, 85, 3902, 0, 0, 0, 0, 0, 0, ...   \n",
       "2232   [14, 11, 2367, 4, 786, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2233   [87, 251, 28, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2234   [3, 139, 1703, 104, 11, 583, 105, 72, 223, 119...   \n",
       "...                                                  ...   \n",
       "12688  [96, 11, 65, 42, 258, 14989, 2, 240, 684, 2, 2...   \n",
       "12689  [3, 504, 494, 3, 4030, 800, 298, 0, 0, 0, 0, 0...   \n",
       "12690  [14, 65, 58, 14990, 164, 33, 1346, 2438, 11, 2...   \n",
       "12691  [307, 20, 1533, 17, 1432, 63, 27, 1, 74, 47, 0...   \n",
       "12692  [94, 20, 628, 12, 6064, 51, 219, 2990, 374, 0,...   \n",
       "\n",
       "                                             mfcc_scaled  \n",
       "2230   [[0.0, 0.37972227268282244, 0.8198979194967001...  \n",
       "2231   [[0.0, 0.5751133741423142, 0.7879647360973706,...  \n",
       "2232   [[0.3555334572333074, 0.6510807576689936, 0.80...  \n",
       "2233   [[0.0, 0.5211673046270286, 0.5221022623447374,...  \n",
       "2234   [[0.2308842054093153, 0.785980178750852, 0.827...  \n",
       "...                                                  ...  \n",
       "12688  [[0.0, 0.44890249617394556, 0.4151811098070448...  \n",
       "12689  [[0.2285113780573238, 0.779642897290782, 0.457...  \n",
       "12690  [[0.0, 0.3924988321574137, 0.6015662496016466,...  \n",
       "12691  [[0.0, 0.404710343900125, 0.6941450242437002, ...  \n",
       "12692  [[0.07021882960247872, 0.2635582968035488, 0.6...  \n",
       "\n",
       "[2567 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6cd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train = data[['temp+eda','text_tokenize','mfcc_scaled']], data['sentiment']\n",
    "x_valid, y_valid = valid[['temp+eda','text_tokenize','mfcc_scaled']], valid['sentiment_x']\n",
    "x_test, y_test = test[['temp+eda','text_tokenize','mfcc_scaled']], t['sentiment_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc484da4",
   "metadata": {},
   "source": [
    "# audio, text 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61c9b2",
   "metadata": {},
   "source": [
    "https://wikidocs.net/103802  참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40d12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class a_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.a_emb = tf.keras.layers.Conv1D(filters=32, kernel_size=551,padding='valid', activation='relu',input_shape=(600,50)) #(1,50,32)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) #(50,32)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 50\n",
    "        \n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.a_emb(tf.convert_to_tensor(x)) #self.a_emb(np.array([x]))[0] #오디오 임베딩\n",
    "        \n",
    "        return x + positions\n",
    "\n",
    "\n",
    "\n",
    "class t_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen,vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.t_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim) #텍스트 임베딩(50*300을 50*1 로 바꿔줘야하는데, 그냥 okt, 케라스 토크나이저 사용해서 쪼개자 그냥 (maxlen=50으로))\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen=50\n",
    "        print(\"t\")\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.t_emb(x) #텍스트 임베딩\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb043a85",
   "metadata": {},
   "source": [
    "# a->t 트랜스포머 (쿼리가 텍스트, 키 밸류는 오디오), 타겟:텍스트, 소스: 오디오\n",
    "\n",
    "텍스트, 오디오 순서 인풋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38be2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class at_MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(at_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio): ##################################################33\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_text)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_text)\n",
    "        key = self.key_dense(inputs_audio)\n",
    "        value = self.value_dense(inputs_audio)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class at_co_TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = at_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio, training):\n",
    "        attn_output = self.att(inputs_text, inputs_audio)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_text + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c668",
   "metadata": {},
   "source": [
    "# ta_트랜스포머  : 오디오 텍스트 순서 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d670886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ta_MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(ta_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text): ##################################################33\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_audio)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_audio)\n",
    "        key = self.key_dense(inputs_text)\n",
    "        value = self.value_dense(inputs_text)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class ta_co_TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = ta_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text, training):\n",
    "        attn_output = self.att(inputs_audio, inputs_text)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_audio + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        print(\"text->au 트랜스포머\")\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c1f1",
   "metadata": {},
   "source": [
    "# bio 트랜스포머\n",
    "#바이오는 co-attetnion이 아니고 그냥 concat해서 input되기때문에 그냥 트랜스포머 block을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0b7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bio_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bio_temp_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #토큰 \n",
    "        self.bio_eda_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #토큰 임베딩\n",
    "        print(self.bio_temp_emb)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 160\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        y1 = self.bio_temp_emb(tf.slice(x,[0,0],[-1,80])) #토큰임베딩 진행\n",
    "        y2= self.bio_eda_emb(tf.slice(x,[0,80],[-1,-1]))\n",
    "\n",
    "        x=tf.concat([y1,y2],axis=1)\n",
    "        \n",
    "\n",
    "        return x + positions\n",
    "\n",
    "class TransformerBlock(layers.Layer):  #공통적인 트랜스포머 블럭\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(embedding_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        #print('어텐션 후',attn_output.shape)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        #print('드롭아웃',attn_output.shape)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        #print('레이어놈',out1.shape)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        #print('ffn후',ffn_output.shape)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        print(\"트랜스포머\")\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c72151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer): #공통적인 트랜스포머 멀티헤드어텐션\n",
    "    def __init__(self, embedding_dim, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789aceb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y_valid = to_categorical(y_valid)\n",
    "y_train = to_categorical(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e72d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import backend as K\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7809e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32 # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=160\n",
    "training=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "159e4f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio (None, 600, 50)\n",
      "t\n",
      "<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f9d317eee50>\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n",
      "트랜스포머\n",
      "(None, 7)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 600, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_1 (TFOpLam (None, 160)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "t__token_and_position_embedding (None, 50, 32)       481312      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "a__token_and_position_embedding (None, 50, 32)       900832      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bio__token_and_position_embeddi (None, 160, 32)      10240       tf.convert_to_tensor_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "at_co__transformer_block_1 (at_ (None, 50, 32)       6464        t__token_and_position_embedding_1\n",
      "                                                                 a__token_and_position_embedding_1\n",
      "__________________________________________________________________________________________________\n",
      "ta_co__transformer_block_1 (ta_ (None, 50, 32)       6464        a__token_and_position_embedding_1\n",
      "                                                                 t__token_and_position_embedding_1\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_2 (Transforme (None, 160, 32)      6464        bio__token_and_position_embedding\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 32)           0           at_co__transformer_block_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           ta_co__transformer_block_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           transformer_block_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_3 (Transforme (None, None, 96)     43904       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 96)           0           transformer_block_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 20)           1940        global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 20)           0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 7)            147         dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,457,767\n",
      "Trainable params: 1,457,767\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio_input = layers.Input(shape=(600,50)) #\n",
    "text_input = layers.Input(shape=(50,)) #\n",
    "bio_input = layers.Input(shape=(160,)) #\n",
    "\n",
    "    #input_audio text bio 는 따로 정해줘야함\n",
    "\n",
    "a_embedding_layer = a_TokenAndPositionEmbedding(600, 32) \n",
    "print(\"audio\",audio_input.shape)\n",
    "audio_embedding = a_embedding_layer(audio_input)######### ################################################################3\n",
    "\n",
    "t_embedding_layer = t_TokenAndPositionEmbedding(50,14991, embed_dim)\n",
    "text_embedding = t_embedding_layer(text_input)  ########## \n",
    "\n",
    "b_embedding_layer = bio_TokenAndPositionEmbedding(160, embed_dim)\n",
    "bio_embedding = b_embedding_layer(tf.convert_to_tensor(bio_input))  ########## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 멀티모달 co-attention\n",
    "at_transformer_block = at_co_TransformerBlock(32, 4, 32)\n",
    "ta_transformer_block = ta_co_TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "bio_transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block = TransformerBlock(96, num_heads, ff_dim)\n",
    "\n",
    "    #진행되는 부분\n",
    "output_at = at_transformer_block(text_embedding, audio_embedding) ############ (50,50,32)\n",
    "output_ta = ta_transformer_block(audio_embedding, text_embedding) # (50,50,32)\n",
    "output_bio = bio_transformer_block(bio_embedding) # (160,160,32)\n",
    "\n",
    "output_at = layers.GlobalAveragePooling1D()(output_at)\n",
    "output_ta = layers.GlobalAveragePooling1D()(output_ta)\n",
    "output_bio = layers.GlobalAveragePooling1D()(output_bio)\n",
    "\n",
    "concat_embedding = layers.concatenate([output_at, output_ta, output_bio])#tf.concat([output_at, output_ta, output_bio], axis=0) #concat 진행\n",
    "\n",
    "output = transformer_block(concat_embedding) \n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(output)\n",
    "x = layers.Dense(20, activation=\"sigmoid\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n",
    "print(outputs.shape)\n",
    "\n",
    "model = keras.Model(inputs=[audio_input, text_input, bio_input], outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', precision, recall, f1score])\n",
    "model.summary()\n",
    "    \n",
    "\n",
    "\n",
    "    #멀티인풋->하나의 아웃풋이어야함\n",
    "    #트랜스포머 블럭별로 하나의 Model 만들어서 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29d87df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 20:52:15.964442: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2937960000 exceeds 10% of free system memory.\n",
      "2023-04-13 20:52:17.245784: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-04-13 20:52:17.248340: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3699850000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "t\n",
      "트랜스포머\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n",
      "t\n",
      "트랜스포머\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 20:52:21.200863: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-13 20:52:21.437985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1528/1531 [============================>.] - ETA: 0s - loss: 1.4897 - accuracy: 0.3874 - precision: 0.7402 - recall: 0.1787 - f1score: 0.2670t\n",
      "트랜스포머\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n",
      "1531/1531 [==============================] - 25s 12ms/step - loss: 1.4888 - accuracy: 0.3878 - precision: 0.7405 - recall: 0.1792 - f1score: 0.2675 - val_loss: 0.9948 - val_accuracy: 0.8254 - val_precision: 0.8283 - val_recall: 0.8188 - val_f1score: 0.8233\n",
      "Epoch 2/5\n",
      "1531/1531 [==============================] - 18s 12ms/step - loss: 0.4067 - accuracy: 0.8981 - precision: 0.9173 - recall: 0.8551 - f1score: 0.8836 - val_loss: 1.1493 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n",
      "Epoch 3/5\n",
      "1531/1531 [==============================] - 17s 11ms/step - loss: 0.2056 - accuracy: 0.9597 - precision: 0.9628 - recall: 0.9533 - f1score: 0.9578 - val_loss: 1.2309 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n",
      "Epoch 4/5\n",
      "1531/1531 [==============================] - 17s 11ms/step - loss: 0.1592 - accuracy: 0.9699 - precision: 0.9718 - recall: 0.9661 - f1score: 0.9689 - val_loss: 1.3496 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n",
      "Epoch 5/5\n",
      "1531/1531 [==============================] - 18s 11ms/step - loss: 0.1638 - accuracy: 0.9680 - precision: 0.9707 - recall: 0.9658 - f1score: 0.9681 - val_loss: 1.2309 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        [[np.array(x_train['mfcc_scaled'].to_list()),np.array(x_train['text_tokenize'].to_list()),np.array(x_train['temp+eda'].to_list())]], y_train, batch_size=16, epochs=5, validation_data=([[np.array(x_valid['mfcc_scaled'].to_list()),np.array(x_valid['text_tokenize'].to_list()),np.array(x_valid['temp+eda'].to_list())]], y_valid)\n",
    "    )\n",
    "#model.save('model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9931e30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _loss, _acc, _precision, _recall, _f1score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate([[np\u001b[38;5;241m.\u001b[39marray(\u001b[43mx_test\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmfcc_scaled\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()),np\u001b[38;5;241m.\u001b[39marray(x_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_tokenize\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()),np\u001b[38;5;241m.\u001b[39marray(x_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp+eda\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list())]], y_test, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, accuracy: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, precision: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, recall: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, f1score: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(_loss, _acc, _precision, _recall, _f1score))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate([[np.array(x_test['mfcc_scaled'].to_list()),np.array(x_test['text_tokenize'].to_list()),np.array(x_test['temp+eda'].to_list())]], y_test, batch_size=16, verbose=1)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc20cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0407",
   "language": "python",
   "name": "uajhmulmo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
