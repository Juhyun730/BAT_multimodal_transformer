{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cf7953",
   "metadata": {},
   "source": [
    "# 여기서부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afd7878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-11 22:43:35.131987: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6b8984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pickle\n",
    "# train\n",
    "with open(\"augmentedhalf1.pkl\",\"rb\") as fr:\n",
    "    data = pickle.load(fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef797fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid.pkl\",\"rb\") as fr:\n",
    "    valid = pickle.load(fr)\n",
    "valid=valid.drop(['Segment ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd6cd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train = data[['temp+eda','text_tokenize','mfcc_scaled']], data['sentiment']\n",
    "x_valid, y_valid = valid[['temp+eda','text_tokenize','mfcc_scaled']], valid['sentiment_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc484da4",
   "metadata": {},
   "source": [
    "# audio, text 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61c9b2",
   "metadata": {},
   "source": [
    "https://wikidocs.net/103802  참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40d12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class a_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.a_emb = tf.keras.layers.Conv1D(filters=32, kernel_size=551,padding='valid', activation='relu',input_shape=(600,50)) #(1,50,32)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) #(50,32)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 50\n",
    "        \n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.a_emb(tf.convert_to_tensor(x)) #self.a_emb(np.array([x]))[0] #오디오 임베딩\n",
    "        \n",
    "        return x + positions\n",
    "\n",
    "\n",
    "\n",
    "class t_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen,vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.t_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim) #텍스트 임베딩(50*300을 50*1 로 바꿔줘야하는데, 그냥 okt, 케라스 토크나이저 사용해서 쪼개자 그냥 (maxlen=50으로))\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen=50\n",
    "        print(\"t\")\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.t_emb(x) #텍스트 임베딩\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb043a85",
   "metadata": {},
   "source": [
    "# a->t 트랜스포머 (쿼리가 텍스트, 키 밸류는 오디오), 타겟:텍스트, 소스: 오디오\n",
    "\n",
    "텍스트, 오디오 순서 인풋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38be2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class at_MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(at_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio): ##################################################33\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_text)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_text)\n",
    "        key = self.key_dense(inputs_audio)\n",
    "        value = self.value_dense(inputs_audio)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class at_co_TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = at_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio, training):\n",
    "        attn_output = self.att(inputs_text, inputs_audio)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_text + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c668",
   "metadata": {},
   "source": [
    "# ta_트랜스포머  : 오디오 텍스트 순서 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d670886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ta_MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(ta_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text): ##################################################33\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_audio)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_audio)\n",
    "        key = self.key_dense(inputs_text)\n",
    "        value = self.value_dense(inputs_text)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class ta_co_TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = ta_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text, training):\n",
    "        attn_output = self.att(inputs_audio, inputs_text)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_audio + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        print(\"text->au 트랜스포머\")\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c1f1",
   "metadata": {},
   "source": [
    "# bio 트랜스포머\n",
    "#바이오는 co-attetnion이 아니고 그냥 concat해서 input되기때문에 그냥 트랜스포머 block을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0b7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bio_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bio_temp_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #토큰 \n",
    "        self.bio_eda_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #토큰 임베딩\n",
    "        print(self.bio_temp_emb)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 160\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        y1 = self.bio_temp_emb(tf.slice(x,[0,0],[-1,80])) #토큰임베딩 진행\n",
    "        y2= self.bio_eda_emb(tf.slice(x,[0,80],[-1,-1]))\n",
    "\n",
    "        x=tf.concat([y1,y2],axis=1)\n",
    "        \n",
    "\n",
    "        return x + positions\n",
    "\n",
    "class TransformerBlock(layers.Layer):  #공통적인 트랜스포머 블럭\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(embedding_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        #print('어텐션 후',attn_output.shape)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        #print('드롭아웃',attn_output.shape)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        #print('레이어놈',out1.shape)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        #print('ffn후',ffn_output.shape)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        print(\"트랜스포머\")\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c72151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer): #공통적인 트랜스포머 멀티헤드어텐션\n",
    "    def __init__(self, embedding_dim, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "789aceb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y_valid = to_categorical(y_valid)\n",
    "y_train = to_categorical(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e72d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import backend as K\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7809e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32 # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=160\n",
    "training=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "159e4f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio (None, 600, 50)\n",
      "t\n",
      "<tensorflow.python.keras.layers.embeddings.Embedding object at 0x7f79a072d790>\n",
      "before trans (None, 50, 32) (None, 50, 32) (None, 160, 32)\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n",
      "after trans (None, 50, 32) (None, 50, 32) (None, 160, 32)\n",
      "after pooling (None, 32) (None, 32) (None, 32)\n",
      "concat (None, 96)\n",
      "트랜스포머\n",
      "output (None, None, 100)\n",
      "(None, 7)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 600, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_1 (TFOpLam (None, 160)          0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "t__token_and_position_embedding (None, 50, 32)       481312      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "a__token_and_position_embedding (None, 50, 32)       900832      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bio__token_and_position_embeddi (None, 160, 32)      10240       tf.convert_to_tensor_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "at_co__transformer_block_1 (at_ (None, 50, 32)       6464        t__token_and_position_embedding_1\n",
      "                                                                 a__token_and_position_embedding_1\n",
      "__________________________________________________________________________________________________\n",
      "ta_co__transformer_block_1 (ta_ (None, 50, 32)       6464        a__token_and_position_embedding_1\n",
      "                                                                 t__token_and_position_embedding_1\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_2 (Transforme (None, 160, 32)      6464        bio__token_and_position_embedding\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 32)           0           at_co__transformer_block_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 32)           0           ta_co__transformer_block_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 32)           0           transformer_block_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 96)           0           global_average_pooling1d_4[0][0] \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_3 (Transforme (None, None, 96)     43904       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, None, 100)    9700        transformer_block_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 100)          0           dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 100)          0           global_average_pooling1d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 50)           5050        dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 50)           0           dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 20)           1020        dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 20)           0           dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 7)            147         dropout_22[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,471,597\n",
      "Trainable params: 1,471,597\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio_input = layers.Input(shape=(600,50)) #\n",
    "text_input = layers.Input(shape=(50,)) #\n",
    "bio_input = layers.Input(shape=(160,)) #\n",
    "\n",
    "    #input_audio text bio 는 따로 정해줘야함\n",
    "\n",
    "a_embedding_layer = a_TokenAndPositionEmbedding(600, 32) \n",
    "print(\"audio\",audio_input.shape)\n",
    "audio_embedding = a_embedding_layer(audio_input)######### ################################################################3\n",
    "\n",
    "t_embedding_layer = t_TokenAndPositionEmbedding(50,14991, embed_dim)\n",
    "text_embedding = t_embedding_layer(text_input)  ########## \n",
    "\n",
    "b_embedding_layer = bio_TokenAndPositionEmbedding(160, embed_dim)\n",
    "bio_embedding = b_embedding_layer(tf.convert_to_tensor(bio_input))  ########## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 멀티모달 co-attention\n",
    "at_transformer_block = at_co_TransformerBlock(32, 4, 32)\n",
    "ta_transformer_block = ta_co_TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "bio_transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block = TransformerBlock(96, num_heads, ff_dim)\n",
    "\n",
    "    #진행되는 부분\n",
    "output_at = at_transformer_block(text_embedding, audio_embedding) ############ (50,50,32)\n",
    "output_ta = ta_transformer_block(audio_embedding, text_embedding) # (50,50,32)\n",
    "output_bio = bio_transformer_block(bio_embedding) # (160,160,32)\n",
    "\n",
    "output_at = layers.GlobalAveragePooling1D()(output_at)\n",
    "output_ta = layers.GlobalAveragePooling1D()(output_ta)\n",
    "output_bio = layers.GlobalAveragePooling1D()(output_bio)\n",
    "\n",
    "concat_embedding = layers.concatenate([output_at, output_ta, output_bio])#tf.concat([output_at, output_ta, output_bio], axis=0) #concat 진행\n",
    "\n",
    "output = transformer_block(concat_embedding) \n",
    "output = layers.Dense(100, activation=\"sigmoid\")(output)\n",
    "\n",
    "output = transformer_block(output)############ concat 후 트랜스포머 블럭\n",
    "\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(output)\n",
    "\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(50, activation=\"sigmoid\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(20, activation=\"sigmoid\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n",
    "print(outputs.shape)\n",
    "\n",
    "model = keras.Model(inputs=[audio_input, text_input, bio_input], outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', precision, recall, f1score])\n",
    "model.summary()\n",
    "    \n",
    "\n",
    "\n",
    "    #멀티인풋->하나의 아웃풋이어야함\n",
    "    #트랜스포머 블럭별로 하나의 Model 만들어서 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29d87df8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "t\n",
      "트랜스포머\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n",
      "t\n",
      "트랜스포머\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n",
      "1531/1531 [==============================] - ETA: 0s - loss: 1.4993 - accuracy: 0.3927 - precision: 0.6901 - recall: 0.2289 - f1score: 0.3170t\n",
      "트랜스포머\n",
      "text->au 트랜스포머\n",
      "트랜스포머\n",
      "1531/1531 [==============================] - 52s 28ms/step - loss: 1.4990 - accuracy: 0.3928 - precision: 0.6902 - recall: 0.2290 - f1score: 0.3171 - val_loss: 1.3315 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n",
      "Epoch 2/5\n",
      "1531/1531 [==============================] - 43s 28ms/step - loss: 0.4310 - accuracy: 0.8527 - precision: 0.8688 - recall: 0.8293 - f1score: 0.8477 - val_loss: 1.6606 - val_accuracy: 0.8223 - val_precision: 0.8193 - val_recall: 0.8180 - val_f1score: 0.8186\n",
      "Epoch 3/5\n",
      "1531/1531 [==============================] - 41s 27ms/step - loss: 0.2514 - accuracy: 0.9351 - precision: 0.9362 - recall: 0.9321 - f1score: 0.9341 - val_loss: 1.6825 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n",
      "Epoch 4/5\n",
      "1531/1531 [==============================] - 39s 26ms/step - loss: 0.1992 - accuracy: 0.9533 - precision: 0.9544 - recall: 0.9520 - f1score: 0.9532 - val_loss: 1.7418 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n",
      "Epoch 5/5\n",
      "1531/1531 [==============================] - 39s 25ms/step - loss: 0.1754 - accuracy: 0.9592 - precision: 0.9606 - recall: 0.9580 - f1score: 0.9593 - val_loss: 1.6677 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        [[np.array(x_train['mfcc_scaled'].to_list()),np.array(x_train['text_tokenize'].to_list()),np.array(x_train['temp+eda'].to_list())]], y_train, batch_size=16, epochs=5, validation_data=([[np.array(x_valid['mfcc_scaled'].to_list()),np.array(x_valid['text_tokenize'].to_list()),np.array(x_valid['temp+eda'].to_list())]], y_valid)\n",
    "    )\n",
    "#model.save('model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9931e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 2s 12ms/step - loss: 1.9388 - accuracy: 0.8029 - precision: 0.8036 - recall: 0.8036 - f1score: 0.8036\n",
      "loss: 1.939, accuracy: 0.803, precision: 0.804, recall: 0.804, f1score: 0.804\n"
     ]
    }
   ],
   "source": [
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate([[np.array(x_test['mfcc_scaled'].to_list()),np.array(x_test['text_tokenize'].to_list()),np.array(x_test['temp+eda'].to_list())]], y_test, batch_size=16, verbose=1)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0407",
   "language": "python",
   "name": "uajhmulmo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
