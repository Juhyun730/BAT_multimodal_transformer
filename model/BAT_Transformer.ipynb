{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 11:10:18.620233: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Bidirectional, MaxPool1D\n",
    "from tensorflow import nn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7e1afa8",
   "metadata": {},
   "source": [
    "- 이때, mfcc_scaled 는 음성의 mfcc feature값을 의미\n",
    "- text_tokenize는 텍스트를 konlpy로 okt()를 사용해 토큰화시키고, 정수화시킨 텍스트 시퀀스를 의미\n",
    "- temp + eda는 temperature 데이터(길이 80;80보다 길면 자르고 짧으면 zero padding), eda 데이터(길이 80;80보다 길면 자르고 짧으면 zero padding)를 concat한 것을 의미함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Segment ID</th>\n",
       "      <th>mfcc_scaled</th>\n",
       "      <th>text_tokenize</th>\n",
       "      <th>temp+eda</th>\n",
       "      <th>sentiment_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sess13_script06_User026F_019</td>\n",
       "      <td>[[0.0, 0.9999999999999999, 0.9608148846764442,...</td>\n",
       "      <td>[331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Sess13_script06_User026F_019</td>\n",
       "      <td>[[0.0, 0.8893497060147615, 0.9251501458319147,...</td>\n",
       "      <td>[331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sess13_script06_User026F_019</td>\n",
       "      <td>[[0.0, 0.9672434656487046, 0.9493295501081099,...</td>\n",
       "      <td>[331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sess13_script06_User026F_019</td>\n",
       "      <td>[[0.0, 0.8324701301043231, 0.8910219154763063,...</td>\n",
       "      <td>[331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Sess13_script06_User026F_019</td>\n",
       "      <td>[[0.0, 0.8692361742952204, 0.8943141198396264,...</td>\n",
       "      <td>[331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24495</th>\n",
       "      <td>7379</td>\n",
       "      <td>Sess35_script04_User070F_008</td>\n",
       "      <td>[[0.032945521331930516, 0.40808203802457316, 0...</td>\n",
       "      <td>[42, 262, 1, 2859, 326, 32, 2859, 100, 344, 11...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24496</th>\n",
       "      <td>7381</td>\n",
       "      <td>Sess31_script06_User061M_026</td>\n",
       "      <td>[[0.0, 0.489686164522752, 0.8910410850529435, ...</td>\n",
       "      <td>[1819, 224, 248, 12, 1140, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24497</th>\n",
       "      <td>7383</td>\n",
       "      <td>Sess34_script02_User068M_020</td>\n",
       "      <td>[[0.0012534706618411162, 0.4571524437803839, 0...</td>\n",
       "      <td>[534, 3934, 3934, 46, 1377, 4, 1618, 0, 0, 0, ...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24498</th>\n",
       "      <td>7387</td>\n",
       "      <td>Sess27_script06_User054M_001</td>\n",
       "      <td>[[0.09887480369794877, 0.5645720231907226, 0.7...</td>\n",
       "      <td>[15, 87, 722, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24499</th>\n",
       "      <td>7392</td>\n",
       "      <td>Sess04_script04_User008F_028</td>\n",
       "      <td>[[0.0, 0.4902733839971465, 0.8978884530814059,...</td>\n",
       "      <td>[44, 11, 383, 1843, 18, 712, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                    Segment ID  \\\n",
       "0          0  Sess13_script06_User026F_019   \n",
       "1          1  Sess13_script06_User026F_019   \n",
       "2          2  Sess13_script06_User026F_019   \n",
       "3          3  Sess13_script06_User026F_019   \n",
       "4          4  Sess13_script06_User026F_019   \n",
       "...      ...                           ...   \n",
       "24495   7379  Sess35_script04_User070F_008   \n",
       "24496   7381  Sess31_script06_User061M_026   \n",
       "24497   7383  Sess34_script02_User068M_020   \n",
       "24498   7387  Sess27_script06_User054M_001   \n",
       "24499   7392  Sess04_script04_User008F_028   \n",
       "\n",
       "                                             mfcc_scaled  \\\n",
       "0      [[0.0, 0.9999999999999999, 0.9608148846764442,...   \n",
       "1      [[0.0, 0.8893497060147615, 0.9251501458319147,...   \n",
       "2      [[0.0, 0.9672434656487046, 0.9493295501081099,...   \n",
       "3      [[0.0, 0.8324701301043231, 0.8910219154763063,...   \n",
       "4      [[0.0, 0.8692361742952204, 0.8943141198396264,...   \n",
       "...                                                  ...   \n",
       "24495  [[0.032945521331930516, 0.40808203802457316, 0...   \n",
       "24496  [[0.0, 0.489686164522752, 0.8910410850529435, ...   \n",
       "24497  [[0.0012534706618411162, 0.4571524437803839, 0...   \n",
       "24498  [[0.09887480369794877, 0.5645720231907226, 0.7...   \n",
       "24499  [[0.0, 0.4902733839971465, 0.8978884530814059,...   \n",
       "\n",
       "                                           text_tokenize  \\\n",
       "0      [331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...   \n",
       "1      [331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...   \n",
       "2      [331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...   \n",
       "3      [331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...   \n",
       "4      [331, 29, 9462, 54, 4, 132, 70, 49, 3392, 9463...   \n",
       "...                                                  ...   \n",
       "24495  [42, 262, 1, 2859, 326, 32, 2859, 100, 344, 11...   \n",
       "24496  [1819, 224, 248, 12, 1140, 0, 0, 0, 0, 0, 0, 0...   \n",
       "24497  [534, 3934, 3934, 46, 1377, 4, 1618, 0, 0, 0, ...   \n",
       "24498  [15, 87, 722, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "24499  [44, 11, 383, 1843, 18, 712, 0, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                                temp+eda  sentiment_x  \n",
       "0      [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            6  \n",
       "1      [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            6  \n",
       "2      [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            6  \n",
       "3      [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            6  \n",
       "4      [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            6  \n",
       "...                                                  ...          ...  \n",
       "24495  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            0  \n",
       "24496  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            0  \n",
       "24497  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            0  \n",
       "24498  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            0  \n",
       "24499  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....            0  \n",
       "\n",
       "[24500 rows x 6 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../audio_augmented_per3500.pkl',\"rb\") as fr: #train 로드\n",
    "    train = pickle.load(fr)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16411/2951617071.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['mfcc_scaled'][i]=np.mean(train['mfcc_scaled'][i],axis=1).reshape(600,1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(train['mfcc_scaled'])):\n",
    "    train['mfcc_scaled'][i]=np.mean(train['mfcc_scaled'][i],axis=1).reshape(600,1) #음성 데이터는 각 시퀀스끼리 mean 연산 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment ID</th>\n",
       "      <th>temp+eda</th>\n",
       "      <th>text_tokenize</th>\n",
       "      <th>mfcc_scaled</th>\n",
       "      <th>sentiment_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sess13_script04_User026F_025</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[70, 5, 9, 476, 239, 10, 75, 76, 29, 36, 62, 1...</td>\n",
       "      <td>[[0.01996256570538113, 0.5113530266302106, 0.8...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sess28_script06_User055M_008</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[42, 3, 2227, 43, 27, 1, 136, 3274, 43, 27, 1,...</td>\n",
       "      <td>[[0.26915004084752314, 0.5238499532268169, 0.6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sess16_script03_User031M_036</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[49, 548, 1895, 17, 145, 14, 53, 659, 385, 13,...</td>\n",
       "      <td>[[0.17421271778196556, 0.4518614135726941, 0.6...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sess07_script04_User014M_022</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[11, 25, 188, 188, 213, 28, 212, 11, 568, 1, 3...</td>\n",
       "      <td>[[0.2582311990368862, 0.5506098383515655, 0.56...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sess10_script03_User020M_006</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[15, 23, 68, 30, 356, 2699, 2, 163, 296, 0, 0,...</td>\n",
       "      <td>[[0.009576183959700835, 0.398568994125834, 0.8...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Sess13_script06_User026F_007</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[58, 736, 260, 2, 2399, 3389, 4213, 3197, 9437...</td>\n",
       "      <td>[[0.0, 0.46085726652280984, 0.7319667002532991...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>Sess01_script04_User002M_023</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[96, 19, 4, 383, 205, 334, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[0.0, 0.5498014295041825, 0.5528216368277, 0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>Sess30_script02_User059F_029</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[105, 15, 148, 224, 502, 30, 3231, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0.05568184697416323, 0.7497303486643664, 0.2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>Sess33_script01_User066M_036</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[13402, 479, 85, 375, 177, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[0.19970357747630973, 0.4442398615317983, 0.7...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Sess05_script06_User010M_002</td>\n",
       "      <td>[35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....</td>\n",
       "      <td>[19, 4, 6, 220, 610, 233, 5106, 7849, 2, 306, ...</td>\n",
       "      <td>[[0.09067940741039338, 0.5531147016030877, 0.7...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1266 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Segment ID  \\\n",
       "0     Sess13_script04_User026F_025   \n",
       "1     Sess28_script06_User055M_008   \n",
       "2     Sess16_script03_User031M_036   \n",
       "3     Sess07_script04_User014M_022   \n",
       "4     Sess10_script03_User020M_006   \n",
       "...                            ...   \n",
       "1261  Sess13_script06_User026F_007   \n",
       "1262  Sess01_script04_User002M_023   \n",
       "1263  Sess30_script02_User059F_029   \n",
       "1264  Sess33_script01_User066M_036   \n",
       "1265  Sess05_script06_User010M_002   \n",
       "\n",
       "                                               temp+eda  \\\n",
       "0     [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "1     [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "2     [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "3     [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "4     [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "...                                                 ...   \n",
       "1261  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "1262  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "1263  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "1264  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "1265  [35.07, 35.07, 35.07, 35.07, 35.07, 35.07, 35....   \n",
       "\n",
       "                                          text_tokenize  \\\n",
       "0     [70, 5, 9, 476, 239, 10, 75, 76, 29, 36, 62, 1...   \n",
       "1     [42, 3, 2227, 43, 27, 1, 136, 3274, 43, 27, 1,...   \n",
       "2     [49, 548, 1895, 17, 145, 14, 53, 659, 385, 13,...   \n",
       "3     [11, 25, 188, 188, 213, 28, 212, 11, 568, 1, 3...   \n",
       "4     [15, 23, 68, 30, 356, 2699, 2, 163, 296, 0, 0,...   \n",
       "...                                                 ...   \n",
       "1261  [58, 736, 260, 2, 2399, 3389, 4213, 3197, 9437...   \n",
       "1262  [96, 19, 4, 383, 205, 334, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1263  [105, 15, 148, 224, 502, 30, 3231, 0, 0, 0, 0,...   \n",
       "1264  [13402, 479, 85, 375, 177, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1265  [19, 4, 6, 220, 610, 233, 5106, 7849, 2, 306, ...   \n",
       "\n",
       "                                            mfcc_scaled  sentiment_x  \n",
       "0     [[0.01996256570538113, 0.5113530266302106, 0.8...            0  \n",
       "1     [[0.26915004084752314, 0.5238499532268169, 0.6...            0  \n",
       "2     [[0.17421271778196556, 0.4518614135726941, 0.6...            0  \n",
       "3     [[0.2582311990368862, 0.5506098383515655, 0.56...            0  \n",
       "4     [[0.009576183959700835, 0.398568994125834, 0.8...            1  \n",
       "...                                                 ...          ...  \n",
       "1261  [[0.0, 0.46085726652280984, 0.7319667002532991...            4  \n",
       "1262  [[0.0, 0.5498014295041825, 0.5528216368277, 0....            0  \n",
       "1263  [[0.05568184697416323, 0.7497303486643664, 0.2...            0  \n",
       "1264  [[0.19970357747630973, 0.4442398615317983, 0.7...            1  \n",
       "1265  [[0.09067940741039338, 0.5531147016030877, 0.7...            0  \n",
       "\n",
       "[1266 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../valid.pkl',\"rb\") as fr: #valid 로드\n",
    "    valid = pickle.load(fr)\n",
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16411/3701760537.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid['mfcc_scaled'][i]=np.mean(valid['mfcc_scaled'][i],axis=1).reshape(600,1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(valid['mfcc_scaled'])):\n",
    "    valid['mfcc_scaled'][i]=np.mean(valid['mfcc_scaled'][i],axis=1).reshape(600,1) #음성 데이터는 각 시퀀스끼리 mean 연산 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../test.pkl',\"rb\") as fr: #test 로드\n",
    "    test = pickle.load(fr)\n",
    "test=test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16411/468462434.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test['mfcc_scaled'][i]=np.mean(test['mfcc_scaled'][i],axis=1).reshape(600,1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(test['mfcc_scaled'])):\n",
    "    test['mfcc_scaled'][i]=np.mean(test['mfcc_scaled'][i],axis=1).reshape(600,1) #음성 데이터는 각 시퀀스끼리 mean 연산 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train['sentiment_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid=valid['sentiment_x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=test['sentiment_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# audio, text 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class a_TokenAndPositionEmbedding(layers.Layer):#오디오 임베딩\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.a1_emb = tf.keras.layers.Conv1D(filters=32, kernel_size=5,padding='valid', activation='relu') #conv1d 연산 2번 진행\n",
    "        self.a2_emb = tf.keras.layers.Conv1D(filters=32, kernel_size=5,padding='valid', activation='relu')\n",
    "        self.a3_emb = MaxPool1D(pool_size=(12),padding='same') #maxpooling 진행\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) \n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 50\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions) #포지셔널 임베딩\n",
    "        x = self.a1_emb(tf.convert_to_tensor(x))\n",
    "        x = self.a2_emb(x)\n",
    "        x = self.a3_emb(x)\n",
    "        \n",
    "        return x + positions #포지셔널 임베딩과 음성 임베딩 값 더해주기\n",
    "\n",
    "class t_TokenAndPositionEmbedding(layers.Layer): #텍스트 임베딩\n",
    "    def __init__(self, maxlen,vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.t_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim) #텍스트 임베딩\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen=50\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.t_emb(x) \n",
    "        return x + positions #포지셔널 임베딩과 텍스트 임베딩 값 더해주기\n",
    "    \n",
    "class bio_TokenAndPositionEmbedding(layers.Layer): #바이오 임베딩\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bio_temp_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #바이오 temp(온도) 임베딩\n",
    "        self.bio_eda_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #바이오 eda 임베딩\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 160\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        y1 = self.bio_temp_emb(tf.slice(x,[0,0],[-1,80])) \n",
    "        y2= self.bio_eda_emb(tf.slice(x,[0,80],[-1,-1]))\n",
    "\n",
    "        x=tf.concat([y1,y2],axis=1) #temp 임베딩과 eda 임베딩은 concat 진행\n",
    "        \n",
    "\n",
    "        return x + positions #포지셔널 임베딩과 바이오 임베딩 값 더해주기\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# audio-text 트랜스포머 block 및 multihead attention (쿼리가 텍스트, 키 밸류는 오디오), 타겟:텍스트, 소스: 오디오\n",
    "\n",
    "텍스트 데이터에 오디오 데이터의 정보가 입혀지는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class at_MultiHeadAttention(tf.keras.layers.Layer):  #오디오->텍스트 멀티헤드어텐션\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(at_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio): \n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_text)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_text)\n",
    "        key = self.key_dense(inputs_audio)\n",
    "        value = self.value_dense(inputs_audio)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class at_co_TransformerBlock(layers.Layer): #audio->text cross attention이 이뤄지는 트랜스포머 블럭\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = at_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio, training):\n",
    "        attn_output = self.att(inputs_text, inputs_audio)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_text + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text-audio 트랜스포머 block 및 multihead attention : (쿼리가 오디오, 키 밸류는 텍스트), 타겟:오디오, 소스: 텍스트\n",
    "오디오 데이터에 텍스트 정보가 입혀지는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ta_MultiHeadAttention(tf.keras.layers.Layer): #텍스트->오디오 멀티헤드어텐션\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(ta_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text): \n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_audio)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_audio)\n",
    "        key = self.key_dense(inputs_text)\n",
    "        value = self.value_dense(inputs_text)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class ta_co_TransformerBlock(layers.Layer): #text->audio cross attention이 이뤄지는 트랜스포머 블럭\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = ta_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text, training):\n",
    "        attn_output = self.att(inputs_audio, inputs_text)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_audio + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트랜스포머\n",
    "바이오는 cross-attetnion이 아니고 그냥 concat해서 input되기때문에 그냥 트랜스포머 block을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):  #공통적인 트랜스포머 블럭\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(embedding_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer): #공통적인 트랜스포머 멀티헤드어텐션\n",
    "    def __init__(self, embedding_dim, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 멀티모달 트랜스포머"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_dim = 32 # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=160\n",
    "training=1\n",
    "\n",
    "audio_input = layers.Input(shape=(600,1))#audio_input = layers.Input(shape=(600,50)) #\n",
    "text_input = layers.Input(shape=(50,)) #\n",
    "bio_input = layers.Input(shape=(160,)) #\n",
    "\n",
    "\n",
    "a_embedding_layer = a_TokenAndPositionEmbedding(50, 32) #오디오 임베딩\n",
    "audio_embedding = a_embedding_layer(audio_input)\n",
    "t_embedding_layer = t_TokenAndPositionEmbedding(50,14991, embed_dim) #텍스트 임베딩\n",
    "text_embedding = t_embedding_layer(text_input)  \n",
    "\n",
    "b_embedding_layer = bio_TokenAndPositionEmbedding(160, embed_dim) #바이오 데이터(temp+eda) 임베딩\n",
    "bio_embedding = b_embedding_layer(tf.convert_to_tensor(bio_input))  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 멀티모달 cross-attention\n",
    "at_transformer_block = at_co_TransformerBlock(32, 4, 32) #audio-text transformer\n",
    "ta_transformer_block = ta_co_TransformerBlock(embed_dim, num_heads, ff_dim) #text-audio transformer\n",
    "bio_transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim) # bio eda,temp transformer\n",
    "transformer_block = TransformerBlock(96, num_heads, ff_dim)\n",
    "\n",
    "\n",
    "#진행되는 부분\n",
    "output_at = at_transformer_block(text_embedding, audio_embedding) #audio-text transformer\n",
    "output_ta = ta_transformer_block(audio_embedding, text_embedding) #text-audio transformer\n",
    "output_bio = bio_transformer_block(bio_embedding) # bio eda,temp transformer\n",
    "output_at = layers.GlobalAveragePooling1D()(output_at)\n",
    "output_ta = layers.GlobalAveragePooling1D()(output_ta)\n",
    "output_bio = layers.GlobalAveragePooling1D()(output_bio)\n",
    "\n",
    "concat_embedding = layers.concatenate([output_at, output_ta, output_bio]) #세가지 모달리티 concat 진행\n",
    "output = transformer_block(concat_embedding)  #공통 트랜스포머 블럭\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(output)\n",
    "x = layers.Dense(80, activation=\"sigmoid\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"sigmoid\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs=[audio_input, text_input, bio_input], outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832d2bfc",
   "metadata": {},
   "source": [
    "# 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1532/1532 [==============================] - 234s 149ms/step - loss: 1.3182 - accuracy: 0.4900 - val_loss: 0.8006 - val_accuracy: 0.8057\n",
      "Epoch 2/5\n",
      "1532/1532 [==============================] - 224s 146ms/step - loss: 0.4780 - accuracy: 0.8678 - val_loss: 0.7403 - val_accuracy: 0.8104\n",
      "Epoch 3/5\n",
      "1532/1532 [==============================] - 210s 137ms/step - loss: 0.2479 - accuracy: 0.9264 - val_loss: 0.7411 - val_accuracy: 0.8144\n",
      "Epoch 4/5\n",
      "1532/1532 [==============================] - 225s 147ms/step - loss: 0.1794 - accuracy: 0.9379 - val_loss: 0.8427 - val_accuracy: 0.7773\n",
      "Epoch 5/5\n",
      "1532/1532 [==============================] - 230s 150ms/step - loss: 0.1366 - accuracy: 0.9496 - val_loss: 0.8469 - val_accuracy: 0.7852\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [[np.array(train['mfcc_scaled'].to_list()),np.array(train['text_tokenize'].to_list()),np.array(train['temp+eda'].to_list())]], y_train, batch_size=16, epochs=5, validation_data=([[np.array(valid['mfcc_scaled'].to_list()),np.array(valid['text_tokenize'].to_list()),np.array(valid['temp+eda'].to_list())]], y_valid)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c71e2f4",
   "metadata": {},
   "source": [
    "# 테스트 데이터셋으로 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 3s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict([np.array(test['mfcc_scaled'].to_list()),np.array(test['text_tokenize'].to_list()),np.array(test['temp+eda'].to_list())])\n",
    "yp=[]\n",
    "for i in y_pred:\n",
    "    yp.append(i.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1906   75   16   42   13    7    3]\n",
      " [ 305   37    2   13    2    2    0]\n",
      " [  26    0    2    1    0    0    0]\n",
      " [  41    2    0    4    1    0    0]\n",
      " [  29    1    0    1    5    0    0]\n",
      " [  13    0    1    0    1    1    0]\n",
      " [   9    0    0    2    0    0    4]]\n",
      "{'0': {'precision': 0.8183769858308286, 'recall': 0.9243452958292919, 'f1-score': 0.868139375996356, 'support': 2062}, '1': {'precision': 0.3217391304347826, 'recall': 0.10249307479224377, 'f1-score': 0.15546218487394958, 'support': 361}, '2': {'precision': 0.09523809523809523, 'recall': 0.06896551724137931, 'f1-score': 0.08, 'support': 29}, '3': {'precision': 0.06349206349206349, 'recall': 0.08333333333333333, 'f1-score': 0.07207207207207209, 'support': 48}, '4': {'precision': 0.22727272727272727, 'recall': 0.1388888888888889, 'f1-score': 0.1724137931034483, 'support': 36}, '5': {'precision': 0.1, 'recall': 0.0625, 'f1-score': 0.07692307692307693, 'support': 16}, '6': {'precision': 0.5714285714285714, 'recall': 0.26666666666666666, 'f1-score': 0.36363636363636365, 'support': 15}, 'accuracy': 0.7631476431632256, 'macro avg': {'precision': 0.3139353676710098, 'recall': 0.23531325382168622, 'f1-score': 0.2555209809436095, 'support': 2567}, 'weighted avg': {'precision': 0.7120389331643536, 'recall': 0.7631476431632256, 'f1-score': 0.7264888635529725, 'support': 2567}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(metrics.confusion_matrix(y_test.to_list(), yp))\n",
    "\n",
    "# Print the precision and recall, among other metrics\n",
    "print(metrics.classification_report(y_test.to_list(), yp, output_dict=True, digits=5)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aa3fb60",
   "metadata": {},
   "source": [
    "증강 전 데이터를 사용하면 오버피팅이 매우 크고, 모델이 중립 감정만 찍는 경향이 있음 \n",
    "\n",
    "- 증강을 통해 모든 감정에 대해 모델이 잘 식별할 수 있도록 학습을 하고, sklearn 의 metrics를 활용하여 모든 감정에 대해 f1score를 구할 수 있었음\n",
    "\n",
    "- test 데이터셋은 랜덤 8개의 세션으로 구성된 데이터 셋임\n",
    "- test 데이터셋의 감정 분포도 중립이 타 감정(ex: 행복)에 비해 매우 많으므로 macro score가 아닌, weigthed score로 평가 진행\n",
    "- 최종스코어는 아래와 같음\n",
    "- 'accuracy': 0.7631476431632256,'precision': 0.7120389331643536, 'recall': 0.7631476431632256, 'f1-score': 0.7264888635529725 의 성능을 낼 수 있었음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bat",
   "language": "python",
   "name": "bat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
