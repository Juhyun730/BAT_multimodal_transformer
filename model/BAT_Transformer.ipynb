{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cf7953",
   "metadata": {},
   "source": [
    "# 여기서부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afd7878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 21:32:00.055998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14883487219160295676\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10764100608\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 3540462853184780759\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10764100608\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 9572449160209115262\n",
      "physical_device_desc: \"device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 21:32:01.966762: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-13 21:32:01.968153: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-04-13 21:32:01.972241: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-13 21:32:02.129068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:02.129476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-04-13 21:32:02.129558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:02.129891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-04-13 21:32:02.129909: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-13 21:32:02.154821: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-13 21:32:02.154897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-04-13 21:32:02.169277: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-13 21:32:02.171940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-13 21:32:02.196790: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-13 21:32:02.199621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-13 21:32:02.243360: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-13 21:32:02.243509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:02.243934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:02.244286: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:02.244614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:02.244902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-04-13 21:32:02.245145: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-13 21:32:03.299377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-13 21:32:03.299400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2023-04-13 21:32:03.299405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2023-04-13 21:32:03.299407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2023-04-13 21:32:03.299971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:03.300374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:03.300728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:03.301086: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:03.301417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2023-04-13 21:32:03.302815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:03.303175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:03.303483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/device:GPU:1 with 10265 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6b8984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "import pickle\n",
    "# train\n",
    "with open(\"augmentedhalf1.pkl\",\"rb\") as fr:\n",
    "    data = pickle.load(fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef797fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"valid.pkl\",\"rb\") as fr:\n",
    "    valid = pickle.load(fr)\n",
    "valid=valid.drop(['Segment ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd48107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.pkl\",\"rb\") as fr:\n",
    "    test = pickle.load(fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6cd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train = data[['temp+eda','text_tokenize','mfcc_scaled']], data['sentiment']\n",
    "x_valid, y_valid = valid[['temp+eda','text_tokenize','mfcc_scaled']], valid['sentiment_x']\n",
    "x_test, y_test = test[['temp+eda','text_tokenize','mfcc_scaled']], test['sentiment_x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc484da4",
   "metadata": {},
   "source": [
    "# audio, text 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61c9b2",
   "metadata": {},
   "source": [
    "https://wikidocs.net/103802  참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40d12b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class a_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "        self.a_emb = tf.keras.layers.Conv1D(filters=32, kernel_size=551,padding='valid', activation='relu',input_shape=(600,50)) #(1,50,32)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim) #(50,32)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 50\n",
    "        \n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.a_emb(tf.convert_to_tensor(x)) #self.a_emb(np.array([x]))[0] #오디오 임베딩\n",
    "        \n",
    "        return x + positions\n",
    "\n",
    "\n",
    "\n",
    "class t_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen,vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.t_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim) #텍스트 임베딩(50*300을 50*1 로 바꿔줘야하는데, 그냥 okt, 케라스 토크나이저 사용해서 쪼개자 그냥 (maxlen=50으로))\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen=50\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        x = self.t_emb(x) #텍스트 임베딩\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb043a85",
   "metadata": {},
   "source": [
    "# a->t 트랜스포머 (쿼리가 텍스트, 키 밸류는 오디오), 타겟:텍스트, 소스: 오디오\n",
    "\n",
    "텍스트, 오디오 순서 인풋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38be2f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class at_MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(at_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio): ##################################################33\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_text)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_text)\n",
    "        key = self.key_dense(inputs_audio)\n",
    "        value = self.value_dense(inputs_audio)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class at_co_TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = at_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_text, inputs_audio, training):\n",
    "        attn_output = self.att(inputs_text, inputs_audio)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_text + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0c668",
   "metadata": {},
   "source": [
    "# ta_트랜스포머  : 오디오 텍스트 순서 input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d670886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ta_MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim, num_heads=8):\n",
    "        super(ta_MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text): ##################################################33\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs_audio)[0]\n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        query = self.query_dense(inputs_audio)\n",
    "        key = self.key_dense(inputs_text)\n",
    "        value = self.value_dense(inputs_text)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs\n",
    "    \n",
    "class ta_co_TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = ta_MultiHeadAttention(embedding_dim=32, num_heads=4)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs_audio, inputs_text, training):\n",
    "        attn_output = self.att(inputs_audio, inputs_text)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs_audio + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        \n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c1f1",
   "metadata": {},
   "source": [
    "# bio 트랜스포머\n",
    "#바이오는 co-attetnion이 아니고 그냥 concat해서 input되기때문에 그냥 트랜스포머 block을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f0b7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class bio_TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bio_temp_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #토큰 \n",
    "        self.bio_eda_emb = layers.Embedding(input_dim=80, output_dim=embed_dim) #토큰 임베딩\n",
    "        \n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = 160\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        y1 = self.bio_temp_emb(tf.slice(x,[0,0],[-1,80])) #토큰임베딩 진행\n",
    "        y2= self.bio_eda_emb(tf.slice(x,[0,80],[-1,-1]))\n",
    "\n",
    "        x=tf.concat([y1,y2],axis=1)\n",
    "        \n",
    "\n",
    "        return x + positions\n",
    "\n",
    "class TransformerBlock(layers.Layer):  #공통적인 트랜스포머 블럭\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(embedding_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        \n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c72151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer): #공통적인 트랜스포머 멀티헤드어텐션\n",
    "    def __init__(self, embedding_dim, num_heads=4):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim # d_model\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        assert embedding_dim % self.num_heads == 0\n",
    "\n",
    "        self.projection_dim = embedding_dim // num_heads\n",
    "        self.query_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embedding_dim)\n",
    "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, query, key, value):\n",
    "        matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "        depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        logits = matmul_qk / tf.math.sqrt(depth)\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        query = self.split_heads(query, batch_size)  \n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        scaled_attention, _ = self.scaled_dot_product_attention(query, key, value)\n",
    "        # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
    "\n",
    "        # (batch_size, seq_len, embedding_dim)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embedding_dim))\n",
    "        outputs = self.dense(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "789aceb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "y_valid = to_categorical(y_valid)\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71e72d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import backend as K\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7809e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32 # Embedding size for each token\n",
    "num_heads = 4  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen=160\n",
    "training=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "159e4f39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio (None, 600, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 21:32:07.433971: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-04-13 21:32:07.434166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.434579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-04-13 21:32:07.434680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.435012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-04-13 21:32:07.435033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-13 21:32:07.435053: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-13 21:32:07.435061: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-04-13 21:32:07.435068: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-13 21:32:07.435075: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-13 21:32:07.435082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-13 21:32:07.435089: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-13 21:32:07.435097: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-13 21:32:07.435127: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.435439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.435751: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.436063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.436387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-04-13 21:32:07.436600: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-04-13 21:32:07.436663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.436974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-04-13 21:32:07.437013: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.437307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce GTX 1080 Ti computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s\n",
      "2023-04-13 21:32:07.437320: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-13 21:32:07.437331: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-13 21:32:07.437338: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-04-13 21:32:07.437345: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-13 21:32:07.437352: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-13 21:32:07.437358: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-13 21:32:07.437365: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-13 21:32:07.437372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-13 21:32:07.437400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.437711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.438041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.438369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.438658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-04-13 21:32:07.438686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-04-13 21:32:07.438690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2023-04-13 21:32:07.438692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N Y \n",
      "2023-04-13 21:32:07.438694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   Y N \n",
      "2023-04-13 21:32:07.438760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.439078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.439424: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.439730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "2023-04-13 21:32:07.439770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-13 21:32:07.440066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10265 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method at_MultiHeadAttention.call of <__main__.at_MultiHeadAttention object at 0x7f1261e53fa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method at_MultiHeadAttention.call of <__main__.at_MultiHeadAttention object at 0x7f1261e53fa0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ta_MultiHeadAttention.call of <__main__.ta_MultiHeadAttention object at 0x7f126160a8b0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ta_MultiHeadAttention.call of <__main__.ta_MultiHeadAttention object at 0x7f126160a8b0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f12605bfd90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MultiHeadAttention.call of <__main__.MultiHeadAttention object at 0x7f12605bfd90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "(None, 7)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 600, 50)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor (TFOpLambd (None, 160)          0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "t__token_and_position_embedding (None, 50, 32)       481312      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "a__token_and_position_embedding (None, 50, 32)       900832      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bio__token_and_position_embeddi (None, 160, 32)      10240       tf.convert_to_tensor[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "at_co__transformer_block (at_co (None, 50, 32)       6464        t__token_and_position_embedding[0\n",
      "                                                                 a__token_and_position_embedding[0\n",
      "__________________________________________________________________________________________________\n",
      "ta_co__transformer_block (ta_co (None, 50, 32)       6464        a__token_and_position_embedding[0\n",
      "                                                                 t__token_and_position_embedding[0\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block (TransformerB (None, 160, 32)      6464        bio__token_and_position_embedding\n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 32)           0           at_co__transformer_block[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 32)           0           ta_co__transformer_block[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 32)           0           transformer_block[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96)           0           global_average_pooling1d[0][0]   \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_1 (Transforme (None, None, 96)     43904       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 96)           0           transformer_block_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 20)           1940        global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20)           0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 7)            147         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,457,767\n",
      "Trainable params: 1,457,767\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "audio_input = layers.Input(shape=(600,50)) #\n",
    "text_input = layers.Input(shape=(50,)) #\n",
    "bio_input = layers.Input(shape=(160,)) #\n",
    "\n",
    "    #input_audio text bio 는 따로 정해줘야함\n",
    "\n",
    "a_embedding_layer = a_TokenAndPositionEmbedding(600, 32) \n",
    "print(\"audio\",audio_input.shape)\n",
    "audio_embedding = a_embedding_layer(audio_input)######### ################################################################3\n",
    "\n",
    "t_embedding_layer = t_TokenAndPositionEmbedding(50,14991, embed_dim)\n",
    "text_embedding = t_embedding_layer(text_input)  ########## \n",
    "\n",
    "b_embedding_layer = bio_TokenAndPositionEmbedding(160, embed_dim)\n",
    "bio_embedding = b_embedding_layer(tf.convert_to_tensor(bio_input))  ########## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 멀티모달 co-attention\n",
    "at_transformer_block = at_co_TransformerBlock(32, 4, 32)\n",
    "ta_transformer_block = ta_co_TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "bio_transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "transformer_block = TransformerBlock(96, num_heads, ff_dim)\n",
    "\n",
    "    #진행되는 부분\n",
    "output_at = at_transformer_block(text_embedding, audio_embedding) ############ (50,50,32)\n",
    "output_ta = ta_transformer_block(audio_embedding, text_embedding) # (50,50,32)\n",
    "output_bio = bio_transformer_block(bio_embedding) # (160,160,32)\n",
    "\n",
    "output_at = layers.GlobalAveragePooling1D()(output_at)\n",
    "output_ta = layers.GlobalAveragePooling1D()(output_ta)\n",
    "output_bio = layers.GlobalAveragePooling1D()(output_bio)\n",
    "\n",
    "concat_embedding = layers.concatenate([output_at, output_ta, output_bio])#tf.concat([output_at, output_ta, output_bio], axis=0) #concat 진행\n",
    "\n",
    "output = transformer_block(concat_embedding) \n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(output)\n",
    "x = layers.Dense(20, activation=\"sigmoid\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "\n",
    "outputs = layers.Dense(7, activation=\"softmax\")(x)  #클래시파이어(분류기)\n",
    "print(outputs.shape)\n",
    "\n",
    "model = keras.Model(inputs=[audio_input, text_input, bio_input], outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', precision, recall, f1score])\n",
    "model.summary()\n",
    "    \n",
    "\n",
    "\n",
    "    #멀티인풋->하나의 아웃풋이어야함\n",
    "    #트랜스포머 블럭별로 하나의 Model 만들어서 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29d87df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 21:32:13.088593: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2937960000 exceeds 10% of free system memory.\n",
      "2023-04-13 21:32:14.480047: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-04-13 21:32:14.482030: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3699850000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-13 21:32:17.854927: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-13 21:32:18.095728: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1531/1531 [==============================] - 24s 12ms/step - loss: 1.5719 - accuracy: 0.3471 - precision: 0.6867 - recall: 0.1407 - f1score: 0.2202 - val_loss: 0.9837 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n",
      "Epoch 2/5\n",
      "1531/1531 [==============================] - 18s 12ms/step - loss: 0.5693 - accuracy: 0.7976 - precision: 0.8665 - recall: 0.6772 - f1score: 0.7526 - val_loss: 1.0937 - val_accuracy: 0.8310 - val_precision: 0.8273 - val_recall: 0.8266 - val_f1score: 0.8269\n",
      "Epoch 3/5\n",
      "1531/1531 [==============================] - 18s 12ms/step - loss: 0.2775 - accuracy: 0.9285 - precision: 0.9375 - recall: 0.9149 - f1score: 0.9256 - val_loss: 1.2792 - val_accuracy: 0.8318 - val_precision: 0.8281 - val_recall: 0.8281 - val_f1score: 0.8281\n",
      "Epoch 4/5\n",
      "1531/1531 [==============================] - 18s 12ms/step - loss: 0.1863 - accuracy: 0.9557 - precision: 0.9599 - recall: 0.9508 - f1score: 0.9552 - val_loss: 1.3699 - val_accuracy: 0.8302 - val_precision: 0.8266 - val_recall: 0.8266 - val_f1score: 0.8266\n",
      "Epoch 5/5\n",
      "1531/1531 [==============================] - 18s 12ms/step - loss: 0.1459 - accuracy: 0.9648 - precision: 0.9692 - recall: 0.9597 - f1score: 0.9643 - val_loss: 1.5158 - val_accuracy: 0.8310 - val_precision: 0.8273 - val_recall: 0.8273 - val_f1score: 0.8273\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "        [[np.array(x_train['mfcc_scaled'].to_list()),np.array(x_train['text_tokenize'].to_list()),np.array(x_train['temp+eda'].to_list())]], y_train, batch_size=16, epochs=5, validation_data=([[np.array(x_valid['mfcc_scaled'].to_list()),np.array(x_valid['text_tokenize'].to_list()),np.array(x_valid['temp+eda'].to_list())]], y_valid)\n",
    "    )\n",
    "#model.save('model.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9931e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 1s 6ms/step - loss: 1.6877 - accuracy: 0.8029 - precision: 0.8036 - recall: 0.8036 - f1score: 0.8036\n",
      "loss: 1.688, accuracy: 0.803, precision: 0.804, recall: 0.804, f1score: 0.804\n"
     ]
    }
   ],
   "source": [
    "_loss, _acc, _precision, _recall, _f1score = model.evaluate([[np.array(x_test['mfcc_scaled'].to_list()),np.array(x_test['text_tokenize'].to_list()),np.array(x_test['temp+eda'].to_list())]], y_test, batch_size=16, verbose=1)\n",
    "print('loss: {:.3f}, accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1score: {:.3f}'.format(_loss, _acc, _precision, _recall, _f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7e0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "0407",
   "language": "python",
   "name": "uajhmulmo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
